(()=>{var oe=Object.create;var te=Object.defineProperty;var se=Object.getOwnPropertyDescriptor;var ae=Object.getOwnPropertyNames;var re=Object.getPrototypeOf,le=Object.prototype.hasOwnProperty;var ue=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports);var he=(e,n,i,o)=>{if(n&&typeof n=="object"||typeof n=="function")for(let s of ae(n))!le.call(e,s)&&s!==i&&te(e,s,{get:()=>n[s],enumerable:!(o=se(n,s))||o.enumerable});return e};var ce=(e,n,i)=>(i=e!=null?oe(re(e)):{},he(n||!e||!e.__esModule?te(i,"default",{value:e,enumerable:!0}):i,e));var ne=ue((exports,module)=>{(function _f(self){"use strict";try{module&&(self=module)}catch(e){}self._factory=_f;var t;function u(e){return typeof e!="undefined"?e:!0}function aa(e){let n=Array(e);for(let i=0;i<e;i++)n[i]=v();return n}function v(){return Object.create(null)}function ba(e,n){return n.length-e.length}function x(e){return typeof e=="string"}function C(e){return typeof e=="object"}function D(e){return typeof e=="function"}function ca(e,n){var i=da;if(e&&(n&&(e=E(e,n)),this.H&&(e=E(e,this.H)),this.J&&1<e.length&&(e=E(e,this.J)),i||i==="")){if(e=e.split(i),this.filter){n=this.filter,i=e.length;let o=[];for(let s=0,r=0;s<i;s++){let l=e[s];l&&!n[l]&&(o[r++]=l)}e=o}return e}return e}let da=/[\p{Z}\p{S}\p{P}\p{C}]+/u,ea=/[\u0300-\u036f]/g;function fa(e,n){let i=Object.keys(e),o=i.length,s=[],r="",l=0;for(let h=0,p,f;h<o;h++)p=i[h],(f=e[p])?(s[l++]=F(n?"(?!\\b)"+p+"(\\b|_)":p),s[l++]=f):r+=(r?"|":"")+p;return r&&(s[l++]=F(n?"(?!\\b)("+r+")(\\b|_)":"("+r+")"),s[l]=""),s}function E(e,n){for(let i=0,o=n.length;i<o&&(e=e.replace(n[i],n[i+1]),e);i+=2);return e}function F(e){return new RegExp(e,"g")}function ha(e){let n="",i="";for(let o=0,s=e.length,r;o<s;o++)(r=e[o])!==i&&(n+=i=r);return n}var ja={encode:ia,F:!1,G:""};function ia(e){return ca.call(this,(""+e).toLowerCase(),!1)}let ka={},G={};function la(e){I(e,"add"),I(e,"append"),I(e,"search"),I(e,"update"),I(e,"remove")}function I(e,n){e[n+"Async"]=function(){let i=this,o=arguments;var s=o[o.length-1];let r;return D(s)&&(r=s,delete o[o.length-1]),s=new Promise(function(l){setTimeout(function(){i.async=!0;let h=i[n].apply(i,o);i.async=!1,l(h)})}),r?(s.then(r),this):s}}function ma(e,n,i,o){let s=e.length,r=[],l,h,p=0;o&&(o=[]);for(let f=s-1;0<=f;f--){let m=e[f],A=m.length,w=v(),k=!l;for(let g=0;g<A;g++){let y=m[g],_=y.length;if(_)for(let B=0,R,q;B<_;B++)if(q=y[B],l){if(l[q]){if(!f){if(i)i--;else if(r[p++]=q,p===n)return r}(f||o)&&(w[q]=1),k=!0}if(o&&(R=(h[q]||0)+1,h[q]=R,R<s)){let $=o[R-2]||(o[R-2]=[]);$[$.length]=q}}else w[q]=1}if(o)l||(h=w);else if(!k)return[];l=w}if(o)for(let f=o.length-1,m,A;0<=f;f--){m=o[f],A=m.length;for(let w=0,k;w<A;w++)if(k=m[w],!l[k]){if(i)i--;else if(r[p++]=k,p===n)return r;l[k]=1}}return r}function na(e,n){let i=v(),o=v(),s=[];for(let r=0;r<e.length;r++)i[e[r]]=1;for(let r=0,l;r<n.length;r++){l=n[r];for(let h=0,p;h<l.length;h++)p=l[h],i[p]&&!o[p]&&(o[p]=1,s[s.length]=p)}return s}function J(e){this.l=e!==!0&&e,this.cache=v(),this.h=[]}function oa(e,n,i){C(e)&&(e=e.query);let o=this.cache.get(e);return o||(o=this.search(e,n,i),this.cache.set(e,o)),o}J.prototype.set=function(e,n){if(!this.cache[e]){var i=this.h.length;for(i===this.l?delete this.cache[this.h[i-1]]:i++,--i;0<i;i--)this.h[i]=this.h[i-1];this.h[0]=e}this.cache[e]=n},J.prototype.get=function(e){let n=this.cache[e];if(this.l&&n&&(e=this.h.indexOf(e))){let i=this.h[e-1];this.h[e-1]=this.h[e],this.h[e]=i}return n};let qa={memory:{charset:"latin:extra",D:3,B:4,m:!1},performance:{D:3,B:3,s:!1,context:{depth:2,D:1}},match:{charset:"latin:extra",G:"reverse"},score:{charset:"latin:advanced",D:20,B:3,context:{depth:3,D:9}},default:{}};function ra(e,n,i,o,s,r,l){setTimeout(function(){let h=e(i?i+"."+o:o,JSON.stringify(l));h&&h.then?h.then(function(){n.export(e,n,i,s,r+1)}):n.export(e,n,i,s,r+1)})}function K(e,n){if(!(this instanceof K))return new K(e);var i;if(e){x(e)?e=qa[e]:(i=e.preset)&&(e=Object.assign({},i[i],e)),i=e.charset;var o=e.lang;x(i)&&(i.indexOf(":")===-1&&(i+=":default"),i=G[i]),x(o)&&(o=ka[o])}else e={};let s,r,l=e.context||{};if(this.encode=e.encode||i&&i.encode||ia,this.register=n||v(),this.D=s=e.resolution||9,this.G=n=i&&i.G||e.tokenize||"strict",this.depth=n==="strict"&&l.depth,this.l=u(l.bidirectional),this.s=r=u(e.optimize),this.m=u(e.fastupdate),this.B=e.minlength||1,this.C=e.boost,this.map=r?aa(s):v(),this.A=s=l.resolution||1,this.h=r?aa(s):v(),this.F=i&&i.F||e.rtl,this.H=(n=e.matcher||o&&o.H)&&fa(n,!1),this.J=(n=e.stemmer||o&&o.J)&&fa(n,!0),i=n=e.filter||o&&o.filter){i=n,o=v();for(let h=0,p=i.length;h<p;h++)o[i[h]]=1;i=o}this.filter=i,this.cache=(n=e.cache)&&new J(n)}t=K.prototype,t.append=function(e,n){return this.add(e,n,!0)},t.add=function(e,n,i,o){if(n&&(e||e===0)){if(!o&&!i&&this.register[e])return this.update(e,n);if(n=this.encode(n),o=n.length){let f=v(),m=v(),A=this.depth,w=this.D;for(let k=0;k<o;k++){let g=n[this.F?o-1-k:k];var s=g.length;if(g&&s>=this.B&&(A||!m[g])){var r=L(w,o,k),l="";switch(this.G){case"full":if(2<s){for(r=0;r<s;r++)for(var h=s;h>r;h--)if(h-r>=this.B){var p=L(w,o,k,s,r);l=g.substring(r,h),M(this,m,l,p,e,i)}break}case"reverse":if(1<s){for(h=s-1;0<h;h--)l=g[h]+l,l.length>=this.B&&M(this,m,l,L(w,o,k,s,h),e,i);l=""}case"forward":if(1<s){for(h=0;h<s;h++)l+=g[h],l.length>=this.B&&M(this,m,l,r,e,i);break}default:if(this.C&&(r=Math.min(r/this.C(n,g,k)|0,w-1)),M(this,m,g,r,e,i),A&&1<o&&k<o-1){for(s=v(),l=this.A,r=g,h=Math.min(A+1,o-k),s[r]=1,p=1;p<h;p++)if((g=n[this.F?o-1-k-p:k+p])&&g.length>=this.B&&!s[g]){s[g]=1;let y=this.l&&g>r;M(this,f,y?r:g,L(l+(o/2>l?0:1),o,k,h-1,p-1),e,i,y?g:r)}}}}}this.m||(this.register[e]=1)}}return this};function L(e,n,i,o,s){return i&&1<e?n+(o||0)<=e?i+(s||0):(e-1)/(n+(o||0))*(i+(s||0))+1|0:0}function M(e,n,i,o,s,r,l){let h=l?e.h:e.map;(!n[i]||l&&!n[i][l])&&(e.s&&(h=h[o]),l?(n=n[i]||(n[i]=v()),n[l]=1,h=h[l]||(h[l]=v())):n[i]=1,h=h[i]||(h[i]=[]),e.s||(h=h[o]||(h[o]=[])),r&&h.includes(s)||(h[h.length]=s,e.m&&(e=e.register[s]||(e.register[s]=[]),e[e.length]=h)))}t.search=function(e,n,i){i||(!n&&C(e)?(i=e,e=i.query):C(n)&&(i=n));let o=[],s,r,l=0;if(i){e=i.query||e,n=i.limit,l=i.offset||0;var h=i.context;r=i.suggest}if(e&&(e=this.encode(""+e),s=e.length,1<s)){i=v();var p=[];for(let m=0,A=0,w;m<s;m++)if((w=e[m])&&w.length>=this.B&&!i[w])if(this.s||r||this.map[w])p[A++]=w,i[w]=1;else return o;e=p,s=e.length}if(!s)return o;n||(n=100),h=this.depth&&1<s&&h!==!1,i=0;let f;h?(f=e[0],i=1):1<s&&e.sort(ba);for(let m,A;i<s;i++){if(A=e[i],h?(m=sa(this,o,r,n,l,s===2,A,f),r&&m===!1&&o.length||(f=A)):m=sa(this,o,r,n,l,s===1,A),m)return m;if(r&&i===s-1){if(p=o.length,!p){if(h){h=0,i=-1;continue}return o}if(p===1)return ta(o[0],n,l)}}return ma(o,n,l,r)};function sa(e,n,i,o,s,r,l,h){let p=[],f=h?e.h:e.map;if(e.s||(f=ua(f,l,h,e.l)),f){let m=0,A=Math.min(f.length,h?e.A:e.D);for(let w=0,k=0,g,y;w<A&&!((g=f[w])&&(e.s&&(g=ua(g,l,h,e.l)),s&&g&&r&&(y=g.length,y<=s?(s-=y,g=null):(g=g.slice(s),s=0)),g&&(p[m++]=g,r&&(k+=g.length,k>=o))));w++);if(m){if(r)return ta(p,o,0);n[n.length]=p;return}}return!i&&p}function ta(e,n,i){return e=e.length===1?e[0]:[].concat.apply([],e),i||e.length>n?e.slice(i,i+n):e}function ua(e,n,i,o){return i?(o=o&&n>i,e=(e=e[o?n:i])&&e[o?i:n]):e=e[n],e}t.contain=function(e){return!!this.register[e]},t.update=function(e,n){return this.remove(e).add(e,n)},t.remove=function(e,n){let i=this.register[e];if(i){if(this.m)for(let o=0,s;o<i.length;o++)s=i[o],s.splice(s.indexOf(e),1);else N(this.map,e,this.D,this.s),this.depth&&N(this.h,e,this.A,this.s);if(n||delete this.register[e],this.cache){n=this.cache;for(let o=0,s,r;o<n.h.length;o++)r=n.h[o],s=n.cache[r],s.includes(e)&&(n.h.splice(o--,1),delete n.cache[r])}}return this};function N(e,n,i,o,s){let r=0;if(e.constructor===Array)if(s)n=e.indexOf(n),n!==-1?1<e.length&&(e.splice(n,1),r++):r++;else{s=Math.min(e.length,i);for(let l=0,h;l<s;l++)(h=e[l])&&(r=N(h,n,i,o,s),o||r||delete e[l])}else for(let l in e)(r=N(e[l],n,i,o,s))||delete e[l];return r}t.searchCache=oa,t.export=function(e,n,i,o,s){let r,l;switch(s||(s=0)){case 0:if(r="reg",this.m){l=v();for(let h in this.register)l[h]=1}else l=this.register;break;case 1:r="cfg",l={doc:0,opt:this.s?1:0};break;case 2:r="map",l=this.map;break;case 3:r="ctx",l=this.h;break;default:return}return ra(e,n||this,i,r,o,s,l),!0},t.import=function(e,n){if(n)switch(x(n)&&(n=JSON.parse(n)),e){case"cfg":this.s=!!n.opt;break;case"reg":this.m=!1,this.register=n;break;case"map":this.map=n;break;case"ctx":this.h=n}},la(K.prototype);function va(e){e=e.data;var n=self._index;let i=e.args;var o=e.task;switch(o){case"init":o=e.options||{},e=e.factory,n=o.encode,o.cache=!1,n&&n.indexOf("function")===0&&(o.encode=Function("return "+n)()),e?(Function("return "+e)()(self),self._index=new self.FlexSearch.Index(o),delete self.FlexSearch):self._index=new K(o);break;default:e=e.id,n=n[o].apply(n,i),postMessage(o==="search"?{id:e,msg:n}:{id:e})}}let wa=0;function O(e){if(!(this instanceof O))return new O(e);var n;e?D(n=e.encode)&&(e.encode=n.toString()):e={},(n=(self||window)._factory)&&(n=n.toString());let i=typeof window=="undefined"&&self.exports,o=this;this.o=xa(n,i,e.worker),this.h=v(),this.o&&(i?this.o.on("message",function(s){o.h[s.id](s.msg),delete o.h[s.id]}):this.o.onmessage=function(s){s=s.data,o.h[s.id](s.msg),delete o.h[s.id]},this.o.postMessage({task:"init",factory:n,options:e}))}P("add"),P("append"),P("search"),P("update"),P("remove");function P(e){O.prototype[e]=O.prototype[e+"Async"]=function(){let n=this,i=[].slice.call(arguments);var o=i[i.length-1];let s;return D(o)&&(s=o,i.splice(i.length-1,1)),o=new Promise(function(r){setTimeout(function(){n.h[++wa]=r,n.o.postMessage({task:e,id:wa,args:i})})}),s?(o.then(s),this):o}}function xa(a,b,c){let d;try{d=b?eval('new (require("worker_threads")["Worker"])("../dist/node/node.js")'):a?new Worker(URL.createObjectURL(new Blob(["onmessage="+va.toString()],{type:"text/javascript"}))):new Worker(x(c)?c:"worker/worker.js",{type:"module"})}catch(e){}return d}function Q(e){if(!(this instanceof Q))return new Q(e);var n=e.document||e.doc||e,i;this.K=[],this.h=[],this.A=[],this.register=v(),this.key=(i=n.key||n.id)&&S(i,this.A)||"id",this.m=u(e.fastupdate),this.C=(i=n.store)&&i!==!0&&[],this.store=i&&v(),this.I=(i=n.tag)&&S(i,this.A),this.l=i&&v(),this.cache=(i=e.cache)&&new J(i),e.cache=!1,this.o=e.worker,this.async=!1,i=v();let o=n.index||n.field||n;x(o)&&(o=[o]);for(let s=0,r,l;s<o.length;s++)r=o[s],x(r)||(l=r,r=r.field),l=C(l)?Object.assign({},e,l):e,this.o&&(i[r]=new O(l),i[r].o||(this.o=!1)),this.o||(i[r]=new K(l,this.register)),this.K[s]=S(r,this.A),this.h[s]=r;if(this.C)for(e=n.store,x(e)&&(e=[e]),n=0;n<e.length;n++)this.C[n]=S(e[n],this.A);this.index=i}function S(e,n){let i=e.split(":"),o=0;for(let s=0;s<i.length;s++)e=i[s],0<=e.indexOf("[]")&&(e=e.substring(0,e.length-2))&&(n[o]=!0),e&&(i[o++]=e);return o<i.length&&(i.length=o),1<o?i:i[0]}function T(e,n){if(x(n))e=e[n];else for(let i=0;e&&i<n.length;i++)e=e[n[i]];return e}function U(e,n,i,o,s){if(e=e[s],o===i.length-1)n[s]=e;else if(e)if(e.constructor===Array)for(n=n[s]=Array(e.length),s=0;s<e.length;s++)U(e,n,i,o,s);else n=n[s]||(n[s]=v()),s=i[++o],U(e,n,i,o,s)}function V(e,n,i,o,s,r,l,h){if(e=e[l])if(o===n.length-1){if(e.constructor===Array){if(i[o]){for(n=0;n<e.length;n++)s.add(r,e[n],!0,!0);return}e=e.join(" ")}s.add(r,e,h,!0)}else if(e.constructor===Array)for(l=0;l<e.length;l++)V(e,n,i,o,s,r,l,h);else l=n[++o],V(e,n,i,o,s,r,l,h)}t=Q.prototype,t.add=function(e,n,i){if(C(e)&&(n=e,e=T(n,this.key)),n&&(e||e===0)){if(!i&&this.register[e])return this.update(e,n);for(let o=0,s,r;o<this.h.length;o++)r=this.h[o],s=this.K[o],x(s)&&(s=[s]),V(n,s,this.A,0,this.index[r],e,s[0],i);if(this.I){let o=T(n,this.I),s=v();x(o)&&(o=[o]);for(let r=0,l,h;r<o.length;r++)if(l=o[r],!s[l]&&(s[l]=1,h=this.l[l]||(this.l[l]=[]),!i||!h.includes(e))&&(h[h.length]=e,this.m)){let p=this.register[e]||(this.register[e]=[]);p[p.length]=h}}if(this.store&&(!i||!this.store[e])){let o;if(this.C){o=v();for(let s=0,r;s<this.C.length;s++)r=this.C[s],x(r)?o[r]=n[r]:U(n,o,r,0,r[0])}this.store[e]=o||n}}return this},t.append=function(e,n){return this.add(e,n,!0)},t.update=function(e,n){return this.remove(e).add(e,n)},t.remove=function(e){if(C(e)&&(e=T(e,this.key)),this.register[e]){for(var n=0;n<this.h.length&&(this.index[this.h[n]].remove(e,!this.o),!this.m);n++);if(this.I&&!this.m)for(let i in this.l){n=this.l[i];let o=n.indexOf(e);o!==-1&&(1<n.length?n.splice(o,1):delete this.l[i])}this.store&&delete this.store[e],delete this.register[e]}return this},t.search=function(e,n,i,o){i||(!n&&C(e)?(i=e,e=""):C(n)&&(i=n,n=0));let s=[],r=[],l,h,p,f,m,A,w=0;if(i)if(i.constructor===Array)p=i,i=null;else{if(e=i.query||e,p=(l=i.pluck)||i.index||i.field,f=i.tag,h=this.store&&i.enrich,m=i.bool==="and",n=i.limit||n||100,A=i.offset||0,f&&(x(f)&&(f=[f]),!e)){for(let g=0,y;g<f.length;g++)(y=ya.call(this,f[g],n,A,h))&&(s[s.length]=y,w++);return w?s:[]}x(p)&&(p=[p])}p||(p=this.h),m=m&&(1<p.length||f&&1<f.length);let k=!o&&(this.o||this.async)&&[];for(let g=0,y,_,B;g<p.length;g++){let R;if(_=p[g],x(_)||(R=_,_=R.field,e=R.query||e,n=R.limit||n),k)k[g]=this.index[_].searchAsync(e,n,R||i);else{if(o?y=o[g]:y=this.index[_].search(e,n,R||i),B=y&&y.length,f&&B){let q=[],$=0;m&&(q[0]=[y]);for(let X=0,ee,H;X<f.length;X++)ee=f[X],(B=(H=this.l[ee])&&H.length)&&($++,q[q.length]=m?[H]:H);$&&(y=m?ma(q,n||100,A||0):na(y,q),B=y.length)}if(B)r[w]=_,s[w++]=y;else if(m)return[]}}if(k){let g=this;return new Promise(function(y){Promise.all(k).then(function(_){y(g.search(e,n,i,_))})})}if(!w)return[];if(l&&(!h||!this.store))return s[0];for(let g=0,y;g<r.length;g++){if(y=s[g],y.length&&h&&(y=za.call(this,y)),l)return y;s[g]={field:r[g],result:y}}return s};function ya(e,n,i,o){let s=this.l[e],r=s&&s.length-i;if(r&&0<r)return(r>n||i)&&(s=s.slice(i,i+n)),o&&(s=za.call(this,s)),{tag:e,result:s}}function za(e){let n=Array(e.length);for(let i=0,o;i<e.length;i++)o=e[i],n[i]={id:o,doc:this.store[o]};return n}t.contain=function(e){return!!this.register[e]},t.get=function(e){return this.store[e]},t.set=function(e,n){return this.store[e]=n,this},t.searchCache=oa,t.export=function(e,n,i,o,s){if(s||(s=0),o||(o=0),o<this.h.length){let r=this.h[o],l=this.index[r];n=this,setTimeout(function(){l.export(e,n,s?r:"",o,s++)||(o++,s=1,n.export(e,n,r,o,s))})}else{let r,l;switch(s){case 1:r="tag",l=this.l;break;case 2:r="store",l=this.store;break;default:return}ra(e,this,i,r,o,s,l)}},t.import=function(e,n){if(n)switch(x(n)&&(n=JSON.parse(n)),e){case"tag":this.l=n;break;case"reg":this.m=!1,this.register=n;for(let o=0,s;o<this.h.length;o++)s=this.index[this.h[o]],s.register=n,s.m=!1;break;case"store":this.store=n;break;default:e=e.split(".");let i=e[0];e=e[1],i&&e&&this.index[i].import(e,n)}},la(Q.prototype);var Ba={encode:Aa,F:!1,G:""};let Ca=[F("[\xE0\xE1\xE2\xE3\xE4\xE5]"),"a",F("[\xE8\xE9\xEA\xEB]"),"e",F("[\xEC\xED\xEE\xEF]"),"i",F("[\xF2\xF3\xF4\xF5\xF6\u0151]"),"o",F("[\xF9\xFA\xFB\xFC\u0171]"),"u",F("[\xFD\u0177\xFF]"),"y",F("\xF1"),"n",F("[\xE7c]"),"k",F("\xDF"),"s",F(" & ")," and "];function Aa(e){var n=e=""+e;return n.normalize&&(n=n.normalize("NFD").replace(ea,"")),ca.call(this,n.toLowerCase(),!e.normalize&&Ca)}var Ea={encode:Da,F:!1,G:"strict"};let Fa=/[^a-z0-9]+/,Ga={b:"p",v:"f",w:"f",z:"s",x:"s",\u00DF:"s",d:"t",n:"m",c:"k",g:"k",j:"k",q:"k",i:"e",y:"e",u:"o"};function Da(e){e=Aa.call(this,e).join(" ");let n=[];if(e){let i=e.split(Fa),o=i.length;for(let s=0,r,l=0;s<o;s++)if((e=i[s])&&(!this.filter||!this.filter[e])){r=e[0];let h=Ga[r]||r,p=h;for(let f=1;f<e.length;f++){r=e[f];let m=Ga[r]||r;m&&m!==p&&(h+=m,p=m)}n[l++]=h}}return n}var Ia={encode:Ha,F:!1,G:""};let Ja=[F("ae"),"a",F("oe"),"o",F("sh"),"s",F("th"),"t",F("ph"),"f",F("pf"),"f",F("(?![aeo])h(?![aeo])"),"",F("(?!^[aeo])h(?!^[aeo])"),""];function Ha(e,n){return e&&(e=Da.call(this,e).join(" "),2<e.length&&(e=E(e,Ja)),n||(1<e.length&&(e=ha(e)),e&&(e=e.split(" ")))),e||[]}var La={encode:Ka,F:!1,G:""};let Ma=F("(?!\\b)[aeo]");function Ka(e){return e&&(e=Ha.call(this,e,!0),1<e.length&&(e=e.replace(Ma,"")),1<e.length&&(e=ha(e)),e&&(e=e.split(" "))),e||[]}G["latin:default"]=ja,G["latin:simple"]=Ba,G["latin:balance"]=Ea,G["latin:advanced"]=Ia,G["latin:extra"]=La;let W=self,Y,Z={Index:K,Document:Q,Worker:O,registerCharset:function(e,n){G[e]=n},registerLanguage:function(e,n){ka[e]=n}};(Y=W.define)&&Y.amd?Y([],function(){return Z}):W.exports?W.exports=Z:W.FlexSearch=Z})(exports)});var ie=ce(ne());var j=document.getElementById("search__text"),z=document.getElementById("search__suggestions");j!==null&&document.addEventListener("keydown",e=>{e.ctrlKey&&e.key==="/"?(e.preventDefault(),j.focus()):e.key==="Escape"&&(j.blur(),z.classList.add("search__suggestions--hidden"))});document.addEventListener("click",e=>{z.contains(e.target)||z.classList.add("search__suggestions--hidden")});document.addEventListener("keydown",e=>{if(z.classList.contains("search__suggestions--hidden"))return;let i=[...z.querySelectorAll("a")];if(i.length===0)return;let o=i.indexOf(document.activeElement);if(e.key==="ArrowDown"){e.preventDefault();let s=o+1<i.length?o+1:o;i[s].focus()}else e.key==="ArrowUp"&&(e.preventDefault(),nextIndex=o>0?o-1:0,i[nextIndex].focus())});(function(){let e=new ie.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/blog/2023/12/10/setting-up-gitea-actions-with-tailscale/",title:"Setting up Gitea Actions with Tailscale",description:`In this post I&rsquo;ll go through the process of setting up Gitea Actions and Tailscale, unlocking a simple and secure way to automate workflows.
`,content:`In this post I&rsquo;ll go through the process of setting up Gitea Actions and Tailscale, unlocking a simple and secure way to automate workflows.
What is Gitea?# Gitea is a lightweight and fast git server that has much of the same look and feel as github. I have been using it in my homelab to mirror repositories hosted on other platforms such as github and gitlab. These mirrors take advantage of the decentralized nature of git by serving as &ldquo;backups&rdquo;. One of the main reasons I hadn&rsquo;t been using it more often was due to the lack of integrated CI/CD. This is no longer the case.
Gitea Actions# Gitea Actions have made it into the 1.19.0 release. This feature had been in an experimental state up until 1.21.0 and is now enabled by default \u{1F389}.
So what are they? If you&rsquo;ve ever used GitHub Actions (and if you&rsquo;re reading this, I imagine you have), these will look familiar. Gitea Actions essentially enable the ability to run github workflows on gitea. Workflows between gitea and github are not completely interopable, but a lot of the same workflow syntax is already compatible on gitea. You can find a documented list of unsupported workflows syntax.
Actions work by using a custom fork of nekos/act. Workflows run in a new container for every job. If you specify an action such as actions/checkout@v4, it defaults to downloading the scripts from github.com. To avoid internet egress, you could always clone the required actions to your local gitea instance.
Actions (gitea&rsquo;s implementation) has me excited because it makes spinning up a network-isolated environment for workflow automation incredibly simple.
Integration with Tailscale# So how does Tailscale help here? Well, more recently I&rsquo;ve been exposing my self-hosted services through a combination of traefik and the tailscale (through the tailscale-traefik proxy integration described here). This allows for a nice looking dns name (i.e. gitea.my-tailnet-name.ts.net) and automatic tls certificate management. I can also share this tailscale node securely with other tailscale users without configuring any firewall rules on my router.
Deploying Gitea, Traefik, and Tailscale# In my case, the following is already set up:
docker-compose is installed tailscale is installed on the gitea host tailscale magic dns is enabled My preferred approach to deploying code in a homelab environment is with docker compose. I have deployed this in a proxmox lxc container based on debian with a hostname gitea. This could be deployed in any environment and with any hostname (as long you updated the tailscale machine name to your preferred subdomain for magic dns).
The docker-compose.yaml file looks like:
version: &#34;3.7&#34; services: gitea: image: gitea/gitea:1.21.1 container_name: gitea environment: - USER_UID=1000 - USER_GID=1000 - GITEA__server__DOMAIN=gitea.my-tailnet-name.ts.net - GITEA__server__ROOT_URL=https://gitea.my-tailnet-name.ts.net - GITEA__server__HTTP_ADDR=0.0.0.0 - GITEA__server__LFS_JWT_SECRET=my-secret-jwt restart: always volumes: - ./data:/data - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro traefik: image: traefik:v3.0.0-beta4 container_name: traefik security_opt: - no-new-privileges:true restart: unless-stopped ports: - 80:80 - 443:443 volumes: - ./traefik/data/traefik.yaml:/traefik.yaml:ro - ./traefik/data/dynamic.yaml:/dynamic.yaml:ro - /var/run/tailscale/tailscaled.sock:/var/run/tailscale/tailscaled.sock traefik/data/traefik.yaml:
entryPoints: https: address: &#34;:443&#34; providers: file: filename: dynamic.yaml certificatesResolvers: myresolver: tailscale: {} log: level: INFO and finally traefik/data/dynamic/dynamic.yaml:
http: routers: gitea: rule: Host(\`gitea.my-tailnet-name.ts.net\`) entrypoints: - &#34;https&#34; service: gitea tls: certResolver: myresolver services: gitea: loadBalancer: servers: - url: &#34;http://gitea:3000&#34; Something to consider is whether or not you want to use ssh with git. One method to get this to work with containers is to use ssh container passthrough. I decided to keep it simple and not use ssh, since communicating over https is perfectly fine for my use case.
After adding the above configuration, running docker compose up -d should be enough to get an instance up and running. It will be accessible at https://gitea.my-tailnet-name.ts.net from within the tailnet.
Theming# I discovered some themes for gitea here and decided to try out gruvbox.
I added the theme by cloning theme-gruvbox-auto.css into ./data/gitea/public/assets/css. I then added the following to environment in docker-compose.yml:
- GITEA__ui__DEFAULT_THEME=gruvbox-auto - GITEA__ui__THEMES=gruvbox-auto After restarting the gitea instance, the default theme was applied.
Connecting runners# I installed the runner by following the docs. I opted for installing it on a separate host (another lxc container) as recommended in the docs. I used the systemd unit file to ensure that the runner comes back online after system reboots. I installed tailscale on this gitea runner as well, so that it can have the same &ldquo;networking privileges&rdquo; as the main instance.
After registering this runner and starting the daemon, the runner appeared in /admin/actions/runners. I added two other runners to help with parallelization.
Running a workflow# Now it&rsquo;s time start running some automation. I used the demo workflow as a starting point to verify that the runner is executing workflows.
After this, I wanted to make sure that some of my existing workflows could be migrated over.
The following workflow uses a matrix to run a job for several of my hosts using ansible playbooks that will do various tasks such as patching os updates and updating container images.
name: Run ansible on: push: schedule: - cron: &#34;0 */12 * * *&#34; jobs: run-ansible-playbook: runs-on: ubuntu-latest strategy: matrix: host: - changedetection - homer - invidious - jackett - jellyfin - ladder - miniflux - plex - qbittorrent - tailscale-exit-node - tailscale-subnet-router - uptime-kuma steps: - name: Check out repository code uses: actions/checkout@v4 - name: Install ansible run: | apt update &amp;&amp; apt install ansible -y - name: Run playbook uses: dawidd6/action-ansible-playbook@v2 with: playbook: playbooks/main.yml requirements: requirements.yml options: | --inventory inventory --limit \${{ matrix.host }} - name: Send failure notification uses: dawidd6/action-send-mail@v3 if: always() &amp;&amp; failure() with: server_address: smtp.gmail.com server_port: 465 secure: true username: myuser password: \${{ secrets.MAIL_PASSWORD }} subject: ansible runbook &#39;\${{ matrix.host }}&#39; failed to: me@davegallant.ca from: RFD Notify body: | \${{ github.server_url }}/\${{ github.repository }}/actions/runs/\${{ github.run_number }} And voil\xE0:
You may be wondering how the gitea runner is allowed to connect to the other hosts using ansible? Well, the nodes are in the same tailnet and have tailscale ssh enabled.
Areas for improvement# One enhancement that I would like to see is the ability to send notifications on workflow failures. Currently, this doesn&rsquo;t seem possible without adding logic to each workflow.
Conclusion# Gitea Actions are fast and the resource footprint is minimal. My gitea instance is currently using around 250mb of memory and a small fraction of a single cpu core (and the runner is using a similar amount of resources). This is impressive since many alternatives tend to require substantially more resources. It likely helps that the codebase is largely written in go.
By combining gitea with the networking marvel that is tailscale, running workflows becomes simple and fun. Whether you are working on a team or working alone, this setup ensures that your workflows are securely accessible from anywhere with an internet connection.
`}).add({id:1,href:"/blog/2023/05/22/using-aks-and-socks-to-connect-to-a-private-azure-db/",title:"Using AKS and SOCKS to connect to a private Azure DB",description:`I ran into a roadblock recently where I wanted to be able to conveniently connect to a managed postgres database within Azure that was not running on public subnets. And by conveniently, I mean that I&rsquo;d rather not have to spin up an ephemeral virtual machine running in the same network and proxy the connection, and I&rsquo;d like to use a local client (preferably with a GUI). After several web searches, it became evident that Azure does not readily provide much tooling to support this.
`,content:`I ran into a roadblock recently where I wanted to be able to conveniently connect to a managed postgres database within Azure that was not running on public subnets. And by conveniently, I mean that I&rsquo;d rather not have to spin up an ephemeral virtual machine running in the same network and proxy the connection, and I&rsquo;d like to use a local client (preferably with a GUI). After several web searches, it became evident that Azure does not readily provide much tooling to support this.
Go Public?# Should the database be migrated to public subnets? Ideally not, since it is good practice to host internal infrastructure in restricted subnets.
How do others handle this?# With GCP, connecting to a private db instance from any machine can be achieved with cloud-sql-proxy. This works by proxying requests from your machine to the SQL database instance in the cloud, while the authentication is handled by GCP&rsquo;s IAM.
So what about Azure? Is there any solution that is as elegant as cloud-sql-proxy?
A Bastion# Similar to what AWS has recommended, perhaps a bastion is the way forward?
Azure has a fully-managed service called Azure Bastion that provides secure access to virtual machines that do not have public IPs. This looks interesting, but unfortunately it costs money and requires an additional virtual machine.
Because this adds cost (and complexity), it does not seem like a desirable option in its current state. If it provided a more seamless connection to the database, it would be more appealing.
SOCKS# 2023-12-13: An alternative to using a socks proxy is socat. This would allow you to relay tcp connections to a pod running in k8s, and then port-forward them to your localhost. If this sounds more appealing, install krew-net-forward and then run &ldquo;kubectl net-forward -i mydb.postgres.database.azure.com -p 5432 -l 5432&rdquo; to access the database through &ldquo;localhost:5432&rdquo;
SOCKS is a protocol that enables a way to proxy connections by exchanging network packets between the client and the server. There are many implementations and many readily available container images that can run a SOCKS server.
It&rsquo;s possible to use this sort of proxy to connect to a private DB, but is it any simpler than using a virtual machine as a jumphost? It wasn&rsquo;t until I stumbled upon kubectl-plugin-socks5-proxy that I was convinced that using SOCKS could be made simple.
So how does it work? By installing the kubectl plugin and then running kubectl socks5-proxy, a SOCKS proxy server is spun up in a pod and then opens up port-forwarding session using kubectl.
As you can see below, this k8s plugin is wrapped up nicely:
$ kubectl socks5-proxy using: namespace=default using: port=1080 using: name=davegallant-proxy using: image=serjs/go-socks5-proxy Creating SOCKS5 Proxy (Pod)... pod/davegallant-proxy created With the above proxy connection open, it is possible to access both the DNS and private IPs accessible within the k8s cluster. In this case, I am able to access the private database, since there is network connectivity between the k8s cluster and the database.
Caveats and Conclusion# The above outlined solution makes some assumptions:
there is a k8s cluster the k8s cluster has network connectivity to the desired private database If these stars align, than this solution might work as a stopgap for accessing a private Azure DB (and I&rsquo;m assuming this could work similarly on AWS).
It would be nice if Azure provided tooling similar to cloud-sql-proxy, so that using private databases would be more of a convenient experience.
One other thing to note is that some clients (such as dbeaver) do not provide DNS resolution over SOCKS. So in this case, you won&rsquo;t be able to use DNS as if you were inside the cluster, but instead have to rely on knowing private ip addresses.
`}).add({id:2,href:"/blog/2022/12/10/watching-youtube-in-private/",title:"Watching YouTube in private",description:`I recently stumbled upon yewtu.be and found it intriguing. It not only allows you to watch YouTube without being on YouTube, but it also allows you to create an account and subscribe to channels without a Google account. What sort of wizardry is going on under the hood? It turns out that it&rsquo;s a hosted instance of invidious.
`,content:`I recently stumbled upon yewtu.be and found it intriguing. It not only allows you to watch YouTube without being on YouTube, but it also allows you to create an account and subscribe to channels without a Google account. What sort of wizardry is going on under the hood? It turns out that it&rsquo;s a hosted instance of invidious.
The layout is simple, and JavaScript is not required.
I started using yewtu.be as my primary client for watching videos. I subscribe to several YouTube channels and I prefer the interface invidiuous provides due to its simplicity. It&rsquo;s also nice to be in control of my search and watch history.
A few days ago, yewtu.be went down briefly, and that motivated me enough to self-host invidious. There are several other hosted instances listed here, but being able to easily backup my own instance (including subscriptions and watch history) is more compelling in my case.
Hosting invidious# The quickest way to get invidious up is with docker-compose as mentioned in the docs.
I made a few modifications, and ended up with:
version: &#34;3&#34; services: invidious: image: quay.io/invidious/invidious restart: unless-stopped ports: - &#34;0.0.0.0:3000:3000&#34; environment: INVIDIOUS_CONFIG: | db: dbname: invidious user: kemal password: kemal host: invidious-db port: 5432 check_tables: true healthcheck: test: wget -nv --tries=1 --spider http://127.0.0.1:3000/api/v1/comments/jNQXAC9IVRw || exit 1 interval: 30s timeout: 5s retries: 2 depends_on: - invidious-db invidious-db: image: docker.io/library/postgres:14 restart: unless-stopped volumes: - postgresdata:/var/lib/postgresql/data - ./config/sql:/config/sql - ./docker/init-invidious-db.sh:/docker-entrypoint-initdb.d/init-invidious-db.sh environment: POSTGRES_DB: invidious POSTGRES_USER: kemal POSTGRES_PASSWORD: kemal healthcheck: test: [&#34;CMD-SHELL&#34;, &#34;pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB&#34;] volumes: postgresdata: After invidious was up and running, I installed Tailscale on it to leverage its MagicDNS, and I&rsquo;m now able to access this instance from anywhere at http://invidious:3000/feed/subscriptions.
Redirecting YouTube links# I figured it would be nice to redirect existing YouTube links that others send me, so that I could seamlessly watch the videos using invidious.
I went looking for a way to redirect paths at the browser level. I found the lightweight proxy requestly, which can be used to modify http requests in my browser. I created the following rules:
Now the link https://www.youtube.com/watch?v=-lz30by8-sU will redirect to http://invidious:3000/watch?v=-lz30by8-sU
I&rsquo;m still looking for ways to improve this invidious setup. There doesn&rsquo;t appear to be a way to stream in 4K yet.
`}).add({id:3,href:"/blog/2022/04/02/virtualizing-my-router-with-pfsense/",title:"Virtualizing my router with pfSense",description:`My aging router has been running OpenWrt for years and for the most part has been quite reliable. OpenWrt is an open-source project used on embedded devices to route network traffic. It supports many different configurations and there exists a large index of packages. Ever since I&rsquo;ve connected some standalone wireless access points, I&rsquo;ve had less of a need for an off-the-shelf all-in-one wireless router combo. I&rsquo;ve also recently been experiencing instability with my router (likely the result of a combination of configuration tweaking and firmware updating). OpenWrt has served me well, but it is time to move on!
`,content:`My aging router has been running OpenWrt for years and for the most part has been quite reliable. OpenWrt is an open-source project used on embedded devices to route network traffic. It supports many different configurations and there exists a large index of packages. Ever since I&rsquo;ve connected some standalone wireless access points, I&rsquo;ve had less of a need for an off-the-shelf all-in-one wireless router combo. I&rsquo;ve also recently been experiencing instability with my router (likely the result of a combination of configuration tweaking and firmware updating). OpenWrt has served me well, but it is time to move on!
pfSense# I figured this would be a good opportunity to try pfSense. I&rsquo;ve heard nothing but positive things about pfSense and the fact it&rsquo;s been around since 2004, based on FreeBSD, and written in PHP gave me the impression that it would be relatively stable (and I&rsquo;d expect nothing less because it has an important job to do!). pfSense can be run on many different machines, and there are even some officially supported appliances. Since I already have a machine running Proxmox, why not just run it in a VM? It&rsquo;d allow for automatic snapshotting of the machine. There is a good video on this by Techno Tim. Tim has a lot of good videos, and this one is about virtualizing pfSense.
Router on a stick# I had initially made the assumption that in order to build a router, you would need more than a single NIC (or a dual-port NIC) in order to support both WAN and LAN. This is simply not the case, because VLANs are awesome! In order to create a router, all you need is a single port NIC and a network switch that supports VLANs (also marketed as a managed switch). I picked up the Netgear GS308E because it has both a sufficient amount of ports for my needs, and it supports VLANs. It also has a nice sturdy metal frame which was a pleasant surprise.
After setting up this Netgear switch, it shoud be possible to access the web interface at http://192.168.0.239. It may be at a different address. To find the address, try checking your DHCP leases in your router interface (if you plugged it into an existing router). I realized I was unable to access this interface because I was on a different subnet, so I set my machine&rsquo;s address to 192.168.0.22 in order to temporarily setup this switch. I assigned a static ip address to the switch (in System &gt; Switch Information) so that it was in the same subnet as the rest of my network.
The web interface is nothing spectactular, but it allows for managing VLANs.
The following configuration will:
assign port 1 to be the LAN (connected to the Proxmox machine) assign port 8 to be the WAN (connected to my ISP&rsquo;s modem) In the switch&rsquo;s web interface, I went to VLAN and then 802.1Q, and then clicked on VLAN Configuration. I configured the ports to look like this:
Note that the VLAN Identifier Setting has been setup already with two VLANs (1 and 10). More VLANs can be created (i.e. to isolate IoT devices), but 2 VLANs is all we need for the initial setup of a router.
To replicate the above configuration, add a new VLAN ID 10 (1 should exist by default).
Next, go into VLAN Membership and configure VLAN 1&rsquo;s port membership to be the following:
and then configure VLAN 10&rsquo;s port membership to be the following:
Now, go into Port PVID and ensure that port 8 is set to PVID 10.
This above configuration will dedicate two of the eight ports to WAN and LAN. This will allow the internet to flow into the pfSense from the modem.
Setting up pfSense# pfSense is fairly easy to setup. Just download the latest ISO and boot up the virtual machine. When setting up the machine, I mostly went with all of the defaults. Configuration can be changed later in the web interface, which is quite a bit simpler.
Since VLANs are going to be leveraged, when you go to Assign Interfaces, VLANs should be setup now like the following:
WAN should be vtnet0.10 LAN should be vtnet0 After going through the rest of the installation, if everything is connected correctly it should display both WAN and LAN addresses.
If all goes well, the web interface should be running at https://192.168.1.1.
And this is where the fun begins. There are many tutorials and blogs about how to setup pfSense and various services and packages that can be installed. I&rsquo;ve already installed pfBlocker-NG.
Summary# It is fairly simple to setup a router with pfSense from within a virtual machine. A physical dedicated routing machine is not necessary and often does not perform as well as software running on faster and more reliable hardware. So far, pfSense has been running for over a week without a single hiccup. pfSense is a mature piece of software that is incredibly powerful and flexible. To avoid some of the instability I had experienced with OpenWrt, I enabled AutoConfigBackup, which is capable of automatically backing up configuration upon every change. I plan to explore and experiment with more services and configuration in the future, so the ability to track all of these changes gives me the peace of mind that experimentation is safe.
`}).add({id:4,href:"/blog/2022/03/13/backing-up-gmail-with-synology/",title:"Backing up gmail with Synology",description:`I&rsquo;ve used gmail since the beta launched touting a whopping 1GB of storage. I thought this was a massive leap in email technology at the time. I was lucky enough to get an invite fairly quickly. Not suprisingly, I have many years of emails, attachments, and photos. I certainly do not want to lose the content of many of these emails. Despite the redundancy of the data that Google secures, I still feel better retaining a copy of this data on my own physical machines.
`,content:`I&rsquo;ve used gmail since the beta launched touting a whopping 1GB of storage. I thought this was a massive leap in email technology at the time. I was lucky enough to get an invite fairly quickly. Not suprisingly, I have many years of emails, attachments, and photos. I certainly do not want to lose the content of many of these emails. Despite the redundancy of the data that Google secures, I still feel better retaining a copy of this data on my own physical machines.
The thought of completely de-googling has crossed my mind on occassion. Convenience, coupled with my admiration for Google engineering, has prevented me from doing so thus far. Though, I may end up doing so at some point in the future.
Synology MailPlus Server# Synology products are reasonably priced for what you get (essentially a cloud-in-a-box) and there is very little maintenance required. I&rsquo;ve recently been in interested in syncing and snapshotting my personal data. I&rsquo;ve setup Synology&rsquo;s Cloud Sync and keep copies of most of my cloud data.
I&rsquo;ve used tools such as gmvault with success in the past. Setting this up on a cron seems like a viable option. However, I don&rsquo;t really need a lot of the features it offers and do not plan to restore this data to another account.
Synology&rsquo;s MailPlus seems to be a good candidate for backing up this data. By enabling POP3 fetching, it&rsquo;s possible to fetch all existing emails, as well as periodically fetch all new emails. If a disaster ever did occur, having these emails would be beneficial, as they are an extension of my memory bank.
Installing MailPlus can be done from the Package Center:
Next, I went into Synology MailPlus Server and on the left, clicked on Account and ensured my user was marked as active.
Afterwords, I followed these instructions in order to start backing up emails.
When entering the POP3 credentials, I created an app password solely for authenticating to POP3 from the Synology device. This is required because I have 2-Step verification enabled on my account. There doesn&rsquo;t seem to be a more secure way to access POP3 at the moment. It does seem like app password access is limited in scope (when MFA is enabled). These app passwords can&rsquo;t be used to login to the main Google account.
I made sure to set the Fetch Range to All in order to get all emails from the beginning of time.
After this, mail started coming in.
After fetching 19 years worth of emails, I tried searching for some emails. It only took a few seconds to search through ~50K emails, which is a relief if I ever did have to search for something important.
Securing Synology# Since Synology devices are not hermetically sealed, it&rsquo;s best to secure them by enabling MFA to help prevent being the victim of ransomware. It is also wise to backup your system settings and volumes to the cloud using a tool such as Hyper Backup. Encrypting your shared volumes should also be done, since unfortunately DSM does not support full disk encryption.
Summary# Having backups of various forms of cloud data is a good investment, especially in times of war. I certainly feel more at ease for having backed up my emails.
`}).add({id:5,href:"/blog/2021/11/14/running-k3s-in-lxc-on-proxmox/",title:"Running K3s in LXC on Proxmox",description:"It has been a while since I&rsquo;ve actively used Kubernetes and wanted to explore the evolution of tools such as Helm and Tekton. I decided to deploy K3s, since I&rsquo;ve had success with deploying it on resource-contrained Raspberry Pis in the past. I thought that this time it&rsquo;d be convenient to have K3s running in a LXC container on Proxmox. This would allow for easy snapshotting of the entire Kubernetes deployment.",content:`It has been a while since I&rsquo;ve actively used Kubernetes and wanted to explore the evolution of tools such as Helm and Tekton. I decided to deploy K3s, since I&rsquo;ve had success with deploying it on resource-contrained Raspberry Pis in the past. I thought that this time it&rsquo;d be convenient to have K3s running in a LXC container on Proxmox. This would allow for easy snapshotting of the entire Kubernetes deployment. LXC containers also provide an efficient way to use a machine&rsquo;s resources.
What is K3s?# K3s is a Kubernetes distro that advertises itself as a lightweight binary with a much smaller memory-footprint than traditional k8s. K3s is not a fork of k8s as it seeks to remain as close to upstream as it possibly can.
Configure Proxmox# This gist contains snippets and discussion on how to deploy K3s in LXC on Proxmox. It mentions that bridge-nf-call-iptables should be loaded, but I did not understand the benefit of doing this.
Disable swap# There is an issue on Kubernetes regarding swap here. There claims to be support for swap in 1.22, but for now let&rsquo;s disable it:
sudo sysctl vm.swappiness=0 sudo swapoff -a It might be worth experimenting with swap enabled in the future to see how that might affect performance.
Enable IP Forwarding# To avoid IP Forwarding issues with Traefik, run the following on the host:
sudo sysctl net.ipv4.ip_forward=1 sudo sysctl net.ipv6.conf.all.forwarding=1 sudo sed -i &#39;s/#net.ipv4.ip_forward=1/net.ipv4.ip_forward=1/g&#39; /etc/sysctl.conf sudo sed -i &#39;s/#net.ipv6.conf.all.forwarding=1/net.ipv6.conf.all.forwarding=1/g&#39; /etc/sysctl.conf Create LXC container# Create an LXC container in the Proxmox interface as you normally would. Remember to:
Uncheck unprivileged container Use a LXC template (I chose a debian 11 template downloaded with pveam) In memory, set swap to 0 Create and start the container Modify container config# Now back on the host run pct list to determine what VMID it was given.
Open /etc/pve/lxc/$VMID.conf and append:
lxc.apparmor.profile: unconfined lxc.cap.drop: lxc.mount.auto: &#34;proc:rw sys:rw&#34; lxc.cgroup2.devices.allow: c 10:200 rwm All of the above configurations are described in the manpages. Notice that cgroup2 is used since Proxmox VE 7.0 has switched to a pure cgroupv2 environment.
Thankfully cgroup v2 support has been supported in k3s with these contributions:
https://github.com/k3s-io/k3s/pull/2584 https://github.com/k3s-io/k3s/pull/2844 Enable shared host mounts# From within the container, run:
echo &#39;#!/bin/sh -e ln -s /dev/console /dev/kmsg mount --make-rshared /&#39; &gt; /etc/rc.local chmod +x /etc/rc.local reboot Install K3s# One of the simplest ways to install K3s on a remote host is to use k3sup. Ensure that you supply a valid CONTAINER_IP and choose the k3s-version you prefer. As of 2021/11, it is still defaulting to the 1.19 channel, so I overrode it to 1.22 for cgroup v2 support. See the published releases here.
ssh-copy-id root@$CONTAINER_IP k3sup install --ip $CONTAINER_IP --user root --k3s-version v1.22.3+k3s1 If all goes well, you should see a path to the kubeconfig generated. I moved this into ~/.kube/config so that kubectl would read this by default.
Wrapping up# Installing K3s in LXC on Proxmox works with a few tweaks to the default configuration. I later followed the Tekton&rsquo;s Getting Started guide and was able to deploy it in a few commands.
$ kubectl get all --namespace tekton-pipelines NAME READY STATUS RESTARTS AGE pod/tekton-pipelines-webhook-8566ff9b6b-6rnh8 1/1 Running 1 (50m ago) 12h pod/tekton-dashboard-6bf858f977-qt4hr 1/1 Running 1 (50m ago) 11h pod/tekton-pipelines-controller-69fd7498d8-f57m4 1/1 Running 1 (50m ago) 12h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/tekton-pipelines-controller ClusterIP 10.43.44.245 &lt;none&gt; 9090/TCP,8080/TCP 12h service/tekton-pipelines-webhook ClusterIP 10.43.183.242 &lt;none&gt; 9090/TCP,8008/TCP,443/TCP,8080/TCP 12h service/tekton-dashboard ClusterIP 10.43.87.97 &lt;none&gt; 9097/TCP 11h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/tekton-pipelines-webhook 1/1 1 1 12h deployment.apps/tekton-dashboard 1/1 1 1 11h deployment.apps/tekton-pipelines-controller 1/1 1 1 12h NAME DESIRED CURRENT READY AGE replicaset.apps/tekton-pipelines-webhook-8566ff9b6b 1 1 1 12h replicaset.apps/tekton-dashboard-6bf858f977 1 1 1 11h replicaset.apps/tekton-pipelines-controller-69fd7498d8 1 1 1 12h NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE horizontalpodautoscaler.autoscaling/tekton-pipelines-webhook Deployment/tekton-pipelines-webhook 9%/100% 1 5 1 12h I made sure to install Tailscale in the container so that I can easily access K3s from anywhere.
If I&rsquo;m feeling adventurous, I might experiment with K3s rootless.
`}).add({id:6,href:"/blog/2021/10/11/replacing-docker-with-podman-on-macos-and-linux/",title:"Replacing docker with podman on macOS (and Linux)",description:`There are a number of reasons why you might want to replace docker, especially on macOS. The following feature bundled in Docker Desktop might have motivated you enough to consider replacing docker:
`,content:`There are a number of reasons why you might want to replace docker, especially on macOS. The following feature bundled in Docker Desktop might have motivated you enough to consider replacing docker:
...ignoring Docker updates is a paid feature now?? pic.twitter.com/ZxKW3b9LQM
&mdash; Brendan Dolan-Gavitt (@moyix) May 1, 2021 Docker has been one of the larger influencers in the container world, helping to standardize the OCI Image Format Specification. For many developers, containers have become synonymous with terms like docker and Dockerfile (a file containing build instructions for a container image). Docker has certainly made it very convenient to build and run containers, but it is not the only solution for doing so.
This post briefly describes my experience swapping out docker for podman on macOS.
What is a container?# A container is a standard unit of software that packages up all application dependencies within it. Multiple containers can be run on a host machine all sharing the same kernel as the host. Linux namespaces help provide an isolated view of the system, including mnt, pid, net, ipc, uid, cgroup, and time. There is an in-depth video that discusses what containers are made from, and near the end there is a demonstration on how to build your own containers from the command line.
By easily allowing the necessary dependencies to live alongside the application code, containers make the &ldquo;works on my machine&rdquo; problem less of a problem.
Benefits of Podman# One of the most interesting features of Podman is that it is daemonless. There isn&rsquo;t a process running on your system managing your containers. In contrast, the docker client is reliant upon the docker daemon (often running as root) to be able to build and run containers.
Podman is rootless by default. It is now possible to run the docker daemon rootless as well, but it&rsquo;s still not the default behaviour.
I&rsquo;ve also observed that so far my 2019 16&quot; Macbook Pro hasn&rsquo;t sounded like a jet engine, although I haven&rsquo;t performed any disk-intensive operations yet.
Installing Podman# Running Podman on macOS is more involved than on Linux, because the podman-machine must run Linux inside of a virtual machine. Fortunately, the installation is made simple with brew (read this if you&rsquo;re installing Podman on Linux):
brew install podman The podman-machine must be started:
# This is not necessary on Linux podman machine init podman machine start Running a container# Let&rsquo;s try to pull an image:
$ podman pull alpine Trying to pull docker.io/library/alpine:latest... Getting image source signatures Copying blob sha256:a0d0a0d46f8b52473982a3c466318f479767577551a53ffc9074c9fa7035982e Copying config sha256:14119a10abf4669e8cdbdff324a9f9605d99697215a0d21c360fe8dfa8471bab Writing manifest to image destination Storing signatures 14119a10abf4669e8cdbdff324a9f9605d99697215a0d21c360fe8dfa8471bab If you&rsquo;re having an issue pulling images, you may need to remove ~/.docker/config.json or remove the set of auths in the configuration as mentioned here.
and then run and exec into the container:
$ podman run --rm -ti alpine Error: error preparing container 99ace1ef8a78118e178372d91fd182e8166c399fbebe0f676af59fbf32ce205b for attach: error configuring network namespace for container 99ace1ef8a78118e178372d91fd182e8166c399fbebe0f676af59fbf32ce205b: error adding pod unruffled_bohr_unruffled_bohr to CNI network &#34;podman&#34;: unexpected end of JSON input What does this error mean? A bit of searching lead to this github issue.
Until the fix is released, a workaround is to just specify a port (even when it&rsquo;s not needed):
podman run -p 4242 --rm -ti alpine If you&rsquo;re reading this from the future, there is a good chance specifying a port won&rsquo;t be needed.
Another example of running a container with Podman can be found in the Jellyfin Documentation.
Aliasing docker with podman# Force of habit (or other scripts) may have you calling docker. To work around this:
alias docker=podman podman-compose# You may be wondering: what about docker-compose? Well, there claims to be a drop-in replacement for it: podman-compose.
pip3 install --user podman-compose Now let&rsquo;s create a docker-compose.yml file to test:
cat &lt;&lt; EOF &gt;&gt; docker-compose.yml version: &#39;2&#39; services: hello_world: image: ubuntu command: [/bin/echo, &#39;Hello world&#39;] EOF Now run:
$ podman-compose up podman pod create --name=davegallant.github.io --share net 40d61dc6e95216c07d2b21cea6dcb30205bfcaf1260501fe652f05bddf7e595e 0 podman create --name=davegallant.github.io_hello_world_1 --pod=davegallant.github.io -l io.podman.compose.config-hash=123 -l io.podman.compose.project=davegallant.github.io -l io.podman.compose.version=0.0.1 -l com.docker.compose.container-number=1 -l com.docker.compose.service=hello_world --add-host hello_world:127.0.0.1 --add-host davegallant.github.io_hello_world_1:127.0.0.1 ubuntu /bin/echo Hello world Resolved &#34;ubuntu&#34; as an alias (/etc/containers/registries.conf.d/000-shortnames.conf) Trying to pull docker.io/library/ubuntu:latest... Getting image source signatures Copying blob sha256:f3ef4ff62e0da0ef761ec1c8a578f3035bef51043e53ae1b13a20b3e03726d17 Copying blob sha256:f3ef4ff62e0da0ef761ec1c8a578f3035bef51043e53ae1b13a20b3e03726d17 Copying config sha256:597ce1600cf4ac5f449b66e75e840657bb53864434d6bd82f00b172544c32ee2 Writing manifest to image destination Storing signatures 1a68b2fed3fdf2037b7aef16d770f22929eec1d799219ce30541df7876918576 0 podman start -a davegallant.github.io_hello_world_1 Hello world This should more or less provide the same results you would come to expect with docker. The README does clearly state that podman-compose is under development.
Summary# Installing Podman on macOS was not seamless, but it was manageable well within 30 minutes. I would recommend giving Podman a try to anyone who is unhappy with experiencing forced docker updates, or who is interested in using a more modern technology for running containers.
One caveat to mention is that there isn&rsquo;t an official graphical user interface for Podman, but there is an open issue considering one. If you rely heavily on Docker Desktop&rsquo;s UI, you may not be as interested in using podman yet.
Update: After further usage, bind mounts do not seem to work out of the box when the client and host are on different machines. A rather involved solution using sshfs was shared here.
I had been experimenting with Podman on Linux before writing this, but after listening to this podcast episode, I was inspired to give Podman a try on macOS.
`}).add({id:7,href:"/blog/2021/09/17/automatically-rotating-aws-access-keys/",title:"Automatically rotating AWS access keys",description:`Rotating credentials is a security best practice. This morning, I read a question about automatically rotating AWS Access Keys without having to go through the hassle of navigating the AWS console. There are some existing solutions already, but I decided to write a script since it was incredibly simple. The script could be packed up as a systemd/launchd service to continually rotate access keys in the background.
In the longer term, migrating my local workflows to aws-vault seems like a more secure solution.`,content:`Rotating credentials is a security best practice. This morning, I read a question about automatically rotating AWS Access Keys without having to go through the hassle of navigating the AWS console. There are some existing solutions already, but I decided to write a script since it was incredibly simple. The script could be packed up as a systemd/launchd service to continually rotate access keys in the background.
In the longer term, migrating my local workflows to aws-vault seems like a more secure solution. This would mean that credentials (even temporary session credentials) never have to be written in plaintext to disk (i.e. where AWS suggests). Any existing applications, such as terraform, could be have their credentials passed to them from aws-vault, which retrieves them from the OS&rsquo;s secure keystore. There is even a rotate command included.
`}).add({id:8,href:"/blog/2021/09/08/why-i-threw-out-my-dotfiles/",title:"Why I threw out my dotfiles",description:`Over the years I have collected a number of dotfiles that I have shared across both Linux and macOS machines (~/.zshrc, ~/.config/git/config, ~/.config/tmux/tmux.conf, etc). I have tried several different ways to manage them, including bare git repos and utilities such as GNU Stow. These solutions work well enough, but I have since found what I would consider a much better solution for organizing user configuration: home-manager.
`,content:`Over the years I have collected a number of dotfiles that I have shared across both Linux and macOS machines (~/.zshrc, ~/.config/git/config, ~/.config/tmux/tmux.conf, etc). I have tried several different ways to manage them, including bare git repos and utilities such as GNU Stow. These solutions work well enough, but I have since found what I would consider a much better solution for organizing user configuration: home-manager.
What is home-manager?# Before understanding home-manager, it is worth briefly discussing what nix is. nix is a package manager that originally spawned from a PhD thesis. Unlike other package managers, it uses symbolic links to keep track of the currently installed packages, keeping around the old ones in case you may want to rollback.
For example, I have used nix to install the package bind which includes dig. You can see that it is available on multiple platforms. The absolute path of dig can be found by running:
$ ls -lh $(which dig) lrwxr-xr-x 73 root 31 Dec 1969 /run/current-system/sw/bin/dig -&gt; /nix/store/0r4qdyprljd3dki57jn6c6a8dh2rbg9g-bind-9.16.16-dnsutils/bin/dig Notice that there is a hash included in the file path? This is a nix store path and is computed by the nix package manager. This nix pill does a good job explaining how this hash is computed. All of the nix pills are worth a read, if you are interested in learning more about nix itself. However, using home-manager does not require extensive knowledge of nix.
Part of the nix ecosystem includes nixpkgs. Many popular tools can be found already packaged in this repository. As you can see with these stats, there is a large number of existing packages that are being maintained by the community. Contributing a new package is easy, and anyone can do it!
home-manager leverages the nix package manager (and nixpkgs), as well the nix language so that you can declaratively define your system configuration. I store my nix-config in git so that I can keep track of my packages and configurations, and retain a clean and informative git commit history so that I can understand what changed and why.
Setting up home-manager# \u26A0\uFE0F If you run this on your main machine, make sure you backup your configuration files first. home-manager is pretty good about not overwriting existing configuration, but it is better to have a backup! Alternatively, you could test this out on a VM or cloud instance.
The first thing you should do is install nix:
curl -L https://nixos.org/nix/install | sh It&rsquo;s generally not a good idea to curl and execute files from the internet (without verifying integrity), so you might want to download the install script first and take a look before executing it!
Open up a new shell in your terminal and running nix should work. If not, run . ~/.nix-profile/etc/profile.d/nix.sh
Now, install home-manager:
nix-channel --add https://github.com/nix-community/home-manager/archive/master.tar.gz home-manager nix-channel --update nix-shell &#39;&lt;home-manager&gt;&#39; -A install You should see a wave of /nix/store/* paths being displayed on your screen.
Now, to start off with a basic configuration, open up ~/.config/nixpkgs/home.nix in the editor of your choice and paste this in (you will want to change userName and homeDirectory):
{ config, pkgs, ... }: { programs.home-manager.enable = true; home = { username = &#34;dave&#34;; homeDirectory = &#34;/home/dave&#34;; stateVersion = &#34;21.11&#34;; packages = with pkgs; [ bind exa fd ripgrep ]; }; programs = { git = { enable = true; aliases = { aa = &#34;add -A .&#34;; br = &#34;branch&#34;; c = &#34;commit -S&#34;; ca = &#34;commit -S --amend&#34;; cb = &#34;checkout -b&#34;; co = &#34;checkout&#34;; d = &#34;diff&#34;; l = &#34;log --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit&#34;; }; delta = { enable = true; options = { features = &#34;line-numbers decorations&#34;; whitespace-error-style = &#34;22 reverse&#34;; plus-style = &#34;green bold ul &#39;#198214&#39;&#34;; decorations = { commit-decoration-style = &#34;bold yellow box ul&#34;; file-style = &#34;bold yellow ul&#34;; file-decoration-style = &#34;none&#34;; }; }; }; extraConfig = { push = { default = &#34;current&#34;; }; pull = { rebase = true; }; }; }; starship = { enable = true; enableZshIntegration = true; settings = { add_newline = false; scan_timeout = 10; }; }; zsh = { enable = true; enableAutosuggestions = true; enableSyntaxHighlighting = true; history.size = 1000000; localVariables = { CASE_SENSITIVE = &#34;true&#34;; DISABLE_UNTRACKED_FILES_DIRTY = &#34;true&#34;; RPROMPT = &#34;&#34;; # override because macOS defaults to filepath ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE = &#34;fg=#838383,underline&#34;; ZSH_DISABLE_COMPFIX = &#34;true&#34;; }; initExtra = &#39;&#39; export PAGER=less &#39;&#39;; shellAliases = { &#34;..&#34; = &#34;cd ..&#34;; grep = &#34;rg --smart-case&#34;; ls = &#34;exa -la --git&#34;; }; &#34;oh-my-zsh&#34; = { enable = true; plugins = [ &#34;gitfast&#34; &#34;last-working-dir&#34; ]; }; }; }; } Save the file and run:
home-manager switch You should see another wave of /nix/store/* paths. The new configuration should now be active.
If you run zsh, you should see that you have starship and access to several other utils such as rg, fd, and exa.
This basic configuration above is also defining your ~/.config/git/config and .zshrc. If you already have either of these files, home-manager will complain about them already existing.
If you run cat ~/.zshrc, you will see the way these configuration files are generated.
You can extend this configuration for programs such as (neo)vim, emacs, alacritty, ssh, etc. To see other programs, take a look at home-manager/modules/programs.
Gateway To Nix# In ways, home-manager can be seen as a gateway to the nix ecosystem. If you have enjoyed the way you can declare user configuration with home-manager, you may be interested in expanding your configuration to include other system dependencies and configuration. For example, in Linux you can define your entire system&rsquo;s configuration (including the kernel, kernel modules, networking, filesystems, etc) in nix. For macOS, there is nix-darwin that includes nix modules for configuring launchd, dock, and other preferences and services. You may also want to check out Nix Flakes: a more recent feature that allows you declare dependencies, and have them automatically pinned and hashed in flake.lock, similar to that of many modern package managers.
Wrapping up# The title of this post is slightly misleading, since it&rsquo;s possible to retain some of your dotfiles and have them intermingle with home-manager by including them alongside nix. The idea of defining user configuration using nix can provide a clean way to maintain your configuration, and allow it to be portable across platforms. Is it worth the effort to migrate away from shell scripts and dotfiles? I&rsquo;d say so.
`}).add({id:9,href:"/blog/2021/09/06/what-to-do-with-a-homelab/",title:"What to do with a homelab",description:`A homelab can be an inexpensive way to host a multitude of internal/external services and learn a lot in the process.
`,content:`A homelab can be an inexpensive way to host a multitude of internal/external services and learn a lot in the process.
Do you want host your own Media server? Ad blocker? Web server? Are you interested in learning more about Linux? Virtualization? Networking? Security? Building a homelab can be an entertaining playground to enhance your computer skills.
One of the best parts about building a homelab is that it doesn&rsquo;t have to be a large investment in terms of hardware. One of the simplest ways to build a homelab is out of a refurbished computer. Having multiple machines/nodes provides the advantage of increased redundancy, but starting out with a single node is enough to reap many of the benefits of having a homelab.
Virtualization# Virtualizing your hardware is an organized way of dividing up your machine&rsquo;s resources. This can be done with something such as a Virtual Machine or something lighter like a container using LXC or runC. Containers have much less overhead in terms of boot time and storage allocation. This Stack Overflow answer sums it up nicely.
A hypervisor such as Proxmox can be installed in minutes on a new machine. It provides a web interface and a straight-forward way to spin up new VMs and containers. Even if your plan is to run mostly docker containers, Proxmox can be a useful abstraction for managing VMs, disks and running scheduled backups. You can even run docker within an LXC container by enabling nested virtualization. You&rsquo;ll want to ensure that VT-d and VT-x are enabled in the BIOS if you decide to install a hypervisor to manage your virtualization.
Services# So what are some useful services to deploy?
Jellyfin or Plex - basically a self-hosted Netflix that can be used to stream from multiple devices, and the best part is that you manage the content! Unlike Plex, Jellyfin is open source and can be found here. changedetection - is a self-hosted equivalent to something like visualping.io that will notify you when a webpage changes and keep track of the diffs Adguard or Pihole - can block a list of known trackers for all clients on your local network. I&rsquo;ve used pihole for a long time, but have recently switched to Adguard since the UI is more modern and it has the ability to toggle on/off a pre-defined list of services, including Netflix (this is useful if you have stealthy young kids). Either of these will speed up your internet experience, simply because you won&rsquo;t need to download all of the extra tracking bloat. Gitea - A lightweight git server. I use this to mirror git repos from GitHub, GitLab, etc. Homer - A customizable landing page for services you need to access (including the ability to quickly search). Uptime Kuma - A fancy tool for monitoring the uptime of services. There is a large number of services you can self-host, including your own applications that you might be developing. awesome-self-hosted provides a curated list of services that might be of interest to you.
VPN# You could certainly setup and manage your own VPN by using something like OpenVPN, but there is also something else you can try: tailscale. It is a very quick way to create fully-encrypted connections between clients. With its MagicDNS, your can reference the names of machines like homer rather than using an IP address. By using this mesh-like VPN, you can easily create a secure tunnel to your homelab from anywhere.
Monitoring# Monitoring can become an important aspect of your homelab after it starts to become something that is relied upon. One of the simplest ways to setup some monitoring is using netdata. It can be installed on individual containers, VMs, and also a hypervisor (such as Proxmox). All of the monitoring works out of the box by detecting disks, memory, network interfaces, etc.
Additionally, agents installed on different machines can all be centrally viewed in netdata, and it can alert you when some of your infrastructure is down or in a degraded state. Adding additional nodes to netdata is as simple as a 1-line shell command.
As mentioned above, Uptime Kuma is a convenient way to track uptime and monitor the availability of your services.
In Summary# Building out a homelab can be a rewarding experience and it doesn&rsquo;t require buying a rack full of expensive servers to get a significant amount of utility. There are many services that you can run that require very minimal setup, making it possible to get a server up and running in a short period of time, with monitoring, and that can be securely connected to remotely.
`}).add({id:10,href:"/blog/2020/03/16/appgate-sdp-on-arch-linux/",title:"AppGate SDP on Arch Linux",description:`AppGate SDP provides a Zero Trust network. This post describes how to get AppGate SDP 4.3.2 working on Arch Linux.
`,content:`AppGate SDP provides a Zero Trust network. This post describes how to get AppGate SDP 4.3.2 working on Arch Linux.
Depending on the AppGate SDP Server that is running, you may require a client that is more recent than the latest package on AUR. As of right now, the latest AUR is 4.2.2-1.
These steps highlight how to get it working with Python3.8 by making a 1 line modification to AppGate source code.
Packaging# We already know the community package is currently out of date, so let&rsquo;s clone it:
git clone https://aur.archlinux.org/appgate-sdp.git cd appgate-sdp You&rsquo;ll likely notice that the version is not what we want, so let&rsquo;s modify the PKGBUILD to the following:
# Maintainer: Pawel Mosakowski &lt;pawel at mosakowski dot net&gt; pkgname=appgate-sdp conflicts=(&#39;appgate-sdp-headless&#39;) pkgver=4.3.2 _download_pkgver=4.3 pkgrel=1 epoch= pkgdesc=&#34;Software Defined Perimeter - GUI client&#34; arch=(&#39;x86_64&#39;) url=&#34;https://www.cyxtera.com/essential-defense/appgate-sdp/support&#34; license=(&#39;custom&#39;) # dependecies calculated by namcap depends=(&#39;gconf&#39; &#39;libsecret&#39; &#39;gtk3&#39; &#39;python&#39; &#39;nss&#39; &#39;libxss&#39; &#39;nodejs&#39; &#39;dnsmasq&#39;) source=(&#34;https://sdpdownloads.cyxtera.com/AppGate-SDP-\${_download_pkgver}/clients/\${pkgname}_\${pkgver}_amd64.deb&#34; &#34;appgatedriver.service&#34;) options=(staticlibs) prepare() { tar -xf data.tar.xz } package() { cp -dpr &#34;\${srcdir}&#34;/{etc,lib,opt,usr} &#34;\${pkgdir}&#34; mv -v &#34;$pkgdir/lib/systemd/system&#34; &#34;$pkgdir/usr/lib/systemd/&#34; rm -vrf &#34;$pkgdir/lib&#34; cp -v &#34;$srcdir/appgatedriver.service&#34; &#34;$pkgdir/usr/lib/systemd/system/appgatedriver.service&#34; mkdir -vp &#34;$pkgdir/usr/share/licenses/appgate-sdp&#34; cp -v &#34;$pkgdir/usr/share/doc/appgate/copyright&#34; &#34;$pkgdir/usr/share/licenses/appgate-sdp&#34; cp -v &#34;$pkgdir/usr/share/doc/appgate/LICENSE.github&#34; &#34;$pkgdir/usr/share/licenses/appgate-sdp&#34; cp -v &#34;$pkgdir/usr/share/doc/appgate/LICENSES.chromium.html.bz2&#34; &#34;$pkgdir/usr/share/licenses/appgate-sdp&#34; } md5sums=(&#39;17101aac7623c06d5fbb95f50cf3dbdc&#39; &#39;002644116e20b2d79fdb36b7677ab4cf&#39;) Let&rsquo;s first make sure we have some dependencies. If you do not have yay, check it out.
yay -S dnsmasq gconf Now, let&rsquo;s install it:
makepkg -si Running the client# Ok, let&rsquo;s run the client by executing appgate.
It complains about not being able to connect.
Easy fix:
sudo systemctl start appgatedriver.service Now we should be connected&hellip; but DNS is not working?
Fixing the DNS# Running resolvectl should display that something is not right.
Why is the DNS not being set by appgate?
$ head -3 /opt/appgate/linux/set_dns #!/usr/bin/env python3 &#39;&#39;&#39; This is used to set and unset the DNS. It seems like python3 is required for the DNS setting to happen. Let&rsquo;s try to run it.
$ sudo /opt/appgate/linux/set_dns /opt/appgate/linux/set_dns:88: SyntaxWarning: &#34;is&#34; with a literal. Did you mean &#34;==&#34;? servers = [( socket.AF_INET if x.version is 4 else socket.AF_INET6, map(int, x.packed)) for x in servers] Traceback (most recent call last): File &#34;/opt/appgate/linux/set_dns&#34;, line 30, in &lt;module&gt; import dbus ModuleNotFoundError: No module named &#39;dbus&#39; Ok, let&rsquo;s install it:
$ sudo python3.8 -m pip install dbus-python Will it work now? Not yet. There&rsquo;s another issue:
$ sudo /opt/appgate/linux/set_dns /opt/appgate/linux/set_dns:88: SyntaxWarning: &#34;is&#34; with a literal. Did you mean &#34;==&#34;? servers = [( socket.AF_INET if x.version is 4 else socket.AF_INET6, map(int, x.packed)) for x in servers] module &#39;platform&#39; has no attribute &#39;linux_distribution&#39; This is a breaking change in Python3.8.
So what is calling platform.linux_distribution?
Let&rsquo;s search for it:
$ sudo grep -r &#39;linux_distribution&#39; /opt/appgate/linux/ /opt/appgate/linux/nm.py: if platform.linux_distribution()[0] != &#39;Fedora&#39;: Aha! So this is in the local AppGate source code. This should be an easy fix. Let&rsquo;s just replace this line with:
if True: # Since we are not using Fedora :) Wrapping up# It turns out there are breaking changes in Python3.8.
The docs say Deprecated since version 3.5, will be removed in version 3.8: See alternative like the distro package.
I suppose this highlights one of the caveats of relying upon the system&rsquo;s python, rather than having an isolated, dedicated environment for all dependencies.
`}),j.addEventListener("input",function(){let i=this.value,o=e.search(i,5,{enrich:!0}),s=new Map;for(let r of o.flatMap(l=>l.result))s.has(r.href)||s.set(r.doc.href,r.doc);if(z.innerHTML="",z.classList.remove("search__suggestions--hidden"),s.size===0&&i){let r=document.createElement("div");r.innerHTML=`No results for "<strong>${i}</strong>"`,r.classList.add("search__no-results"),z.appendChild(r);return}for(let[r,l]of s){let h=document.createElement("a");h.href=r,h.classList.add("search__suggestion-item"),z.appendChild(h);let p=document.createElement("div");p.textContent=l.title,p.classList.add("search__suggestion-title"),h.appendChild(p);let f=document.createElement("div");if(f.textContent=l.description,f.classList.add("search__suggestion-description"),h.appendChild(f),z.childElementCount===5)break}})})();})();
//! Source: https://github.com/h-enk/doks/blob/master/assets/js/index.js
/*! Source: https://dev.to/shubhamprakash/trap-focus-using-javascript-6a3 */
//! Source: https://discourse.gohugo.io/t/range-length-or-last-element/3803/2
