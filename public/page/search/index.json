[{"content":"I\u0026rsquo;m a software engineer with a passion for open-source, infrastructure, tooling and security.\n","href":"/about/","title":"About"},{"content":"","href":"/","title":"davegallant"},{"content":"","href":"/tags/openwrt/","title":"openwrt"},{"content":"","href":"/tags/pfsense/","title":"pfsense"},{"content":"","href":"/post/","title":"Posts"},{"content":"","href":"/tags/proxmox/","title":"proxmox"},{"content":"","href":"/tags/router/","title":"router"},{"content":"","href":"/tags/router-on-a-stick/","title":"router-on-a-stick"},{"content":"","href":"/tags/","title":"Tags"},{"content":"The problem My aging router has been running OpenWrt for years and for the most part has been quite reliable. OpenWrt is an open-source project used on embedded devices to route network traffic. It supports many different configurations and there exists a large index of packages. Ever since I\u0026rsquo;ve connected some standalone wireless access points, I\u0026rsquo;ve had less of a need for an off-the-shelf all-in-one wireless router combo. I\u0026rsquo;ve also recently been experiencing instability with my router (likely the result of a combination of setting tweaking and firmware updating). OpenWrt has served me well, but it is time to move on!\npfSense I figured this would be a good opportunity to try pfSense. I\u0026rsquo;ve heard nothing but positive things about pfSense and the fact it\u0026rsquo;s been around since 2004, based on FreeBSD, and written in PHP gave me the impression that it would be relatively stable (and I\u0026rsquo;d expect nothing less because it has an important job to do!). pfSense can be run on many different machines, and there are even some officially supported appliances. Since I already have a machine running Proxmox, why not just run it in a VM? It\u0026rsquo;d allow for automatic snapshotting of the machine. There is a good video on this by Techno Tim. Tim has a lot of good videos, and this one is about virtualizing pfSense.\nRouter on a stick I had initially made the assumption that in order to build a router, you would need more than a single NIC (or a dual-port NIC) in order to support both WAN and LAN. This is simply not the case, because VLANs are awesome! In order to create a router, all you need is a single port NIC and a network switch that supports VLANs (also marketed as a managed switch). I picked up the Netgear GS308E because it has both a sufficient amount of ports for my needs, and it supports VLANs. It also has a nice sturdy metal frame which was a pleasant surprise.\nAfter setting up this Netgear switch, it shoud be possible to access the web interface at http://192.168.0.239. It may be at a different address. To find the address, try checking your DHCP leases in your router interface (if you plugged it into an existing router). I realized I was unable to access this interface because I was on a different subnet, so I set my machine\u0026rsquo;s address to 192.168.0.22 in order to temporarily setup this switch. I assigned a static ip address to the switch (in System \u0026gt; Switch Information) so that it was in the same subnet as the rest of my network.\nThe web interface is nothing spectactular, but it allows for managing VLANs.\nThe following configuration will:\nassign port 1 to be the LAN (connected to the Proxmox machine) assign port 8 to be the WAN (connected to my ISP\u0026rsquo;s modem) In the switch\u0026rsquo;s web interface, I went to VLAN and then 802.1Q, and then clicked on VLAN Configuration. I configured the ports to look like this:\nNote that the VLAN Identifier Setting has been setup already with two VLANs (1 and 10). More VLANs can be created (i.e. to isolate IoT devices), but 2 VLANs is all we need for the initial setup of a router.\nTo replicate the above configuration, add a new VLAN ID 10 (1 should exist by default).\nNext, go into VLAN Membership and configure VLAN 1\u0026rsquo;s port membership to be the following:\nand then configure VLAN 10\u0026rsquo;s port membership to be the following:\nNow, go into Port PVID and ensure that port 8 is set to PVID 10.\nThis above configuration will dedicate two of the eight ports to WAN and LAN. This will allow the internet to flow into the pfSense from the modem.\nSetting up pfSense pfSense is fairly easy to setup. Just download the latest ISO and boot up the virtual machine. When setting up the machine, I mostly went with all of the defaults. Configuration can be changed later in the web interface, which is quite a bit simpler.\nSince VLANs are going to be leveraged, when you go to Assign Interfaces, VLANs should be setup now like the following:\nWAN should be vtnet0.10 LAN should be vtnet0 After going through the rest of the installation, if everything is connected correctly it should display both WAN and LAN addresses.\nIf all goes well, the web interface should be running at https://192.168.1.1.\nAnd this is where the fun begins. There are many tutorials and blogs about how to setup pfSense and various services and packages that can be installed. I\u0026rsquo;ve already installed pfBlocker-NG.\nSummary It is fairly simple to setup a router with pfSense from within a virtual machine. A physical dedicated routing machine is not necessary and often does not perform as well as software running on faster and more reliable hardware. So far, pfSense has been running for over a week without a single hiccup. pfSense is a mature piece of software that is incredibly powerful and flexible. To avoid some of the instability I had experienced with OpenWrt, I enabled AutoConfigBackup, which is capable of automatically backing up configuration upon every change. I plan to explore and experiment with more services and configuration in the future, so the ability to track all of these changes gives me the peace of mind that experimentation is safe.\n","href":"/blog/2022/04/02/virtualizing-my-router-with-pfsense/","title":"Virtualizing My Router With pfSense"},{"content":"","href":"/tags/vlan/","title":"vlan"},{"content":"I\u0026rsquo;ve used gmail since the beta launched touting a whopping 1GB of storage. I thought this was a massive leap in email technology at the time. I was lucky enough to get an invite fairly quickly. Not suprisingly, I have many years of emails, attachments, and photos. I certainly do not want to lose the content of many of these emails. Despite the redundancy of the data that Google secures, I still feel better retaining a copy of this data on my own physical machines.\nThe thought of completely de-googling has crossed my mind on occassion. The convenience coupled with my admiration for their engineering has prevented me from doing so thus far. Though, I may end up doing so at some point in the future.\nSynology MailPlus Server Synology products are reasonably priced for what you get (essentially a cloud-in-a-box) and there is very little maintenance required. I\u0026rsquo;ve recently been in interested in syncing and snapshotting my personal data. I\u0026rsquo;ve setup Synology\u0026rsquo;s Cloud Sync and keep copies of most of my cloud data.\nI\u0026rsquo;ve used tools such as gmvault with success in the past. Setting this up on a cron seems like a viable option. However, I don\u0026rsquo;t really need a lot of the features it offers and do not plan to restore this data to another account.\nSynology\u0026rsquo;s MailPlus seems to be a good candidate for backing up this data. By enabling POP3 fetching, it\u0026rsquo;s possible to fetch all existing emails, as well as periodically fetch all new emails. If a disaster ever did occur, having these emails would be beneficial, as they are an extension of my memory bank.\nInstalling MailPlus can be done from the Package Center:\nNext, I went into Synology MailPlus Server and on the left, clicked on Account and ensured my user was marked as active.\nAfterwords, I followed these instructions in order to start backing up emails.\nWhen entering the POP3 credentials, I created an app password solely for authenticating to POP3 from the Synology device. This is required because I have 2-Step verification enabled on my account. There doesn\u0026rsquo;t seem to be a more secure way to access POP3 at the moment. It does seem like app password access is limited in scope (when MFA is enabled). These app passwords can\u0026rsquo;t be used to login to the main Google account.\nI made sure to set the Fetch Range to All in order to get all emails from the beginning of time.\nAfter this, mail started coming in.\nAfter fetching 19 years worth of emails, I tried searching for some emails. It only took a few seconds to search through ~50K emails, which is a relief if I ever did have to search for something important.\nSecuring Synology Since Synology devices are not hermetically sealed, it\u0026rsquo;s best to secure them by enabling MFA to help prevent being the victim of ransomware. It is also wise to backup your system settings and volumes to the cloud using a tool such as Hyper Backup. Encrypting your shared volumes should also be done, since unfortunately DSM does not support full disk encryption.\nSummary Having backups of various forms of cloud data is a good investment, especially in times of war. I certainly feel more at ease for having backed up my emails.\n","href":"/blog/2022/03/13/backing-up-gmail-with-synology/","title":"Backing Up Gmail With Synology"},{"content":"","href":"/tags/backup/","title":"backup"},{"content":"","href":"/tags/degoogle/","title":"degoogle"},{"content":"","href":"/tags/gmail/","title":"gmail"},{"content":"","href":"/tags/ransomware/","title":"ransomware"},{"content":"","href":"/tags/synology/","title":"synology"},{"content":"","href":"/tags/k3s/","title":"k3s"},{"content":"","href":"/tags/lxc/","title":"lxc"},{"content":"It has been a while since I\u0026rsquo;ve actively used Kubernetes and wanted to explore the evolution of tools such as Helm and Tekton. I decided to deploy K3s, since I\u0026rsquo;ve had success with deploying it on resource-contrained Raspberry Pis in the past. I thought that this time it\u0026rsquo;d be convenient to have K3s running in a LXC container on Proxmox. This would allow for easy snapshotting of the entire Kubernetes deployment. LXC containers also provide an efficient way to use a machine\u0026rsquo;s resources.\nWhat is K3s? K3s is a Kubernetes distro that advertises itself as a lightweight binary with a much smaller memory-footprint than traditional k8s. K3s is not a fork of k8s as it seeks to remain as close to upstream as it possibly can.\nConfigure Proxmox This gist contains snippets and discussion on how to deploy K3s in LXC on Proxmox. It mentions that bridge-nf-call-iptables should be loaded, but I did not understand the benefit of doing this.\nDisable swap There is an issue on Kubernetes regarding swap here. There claims to be support for swap in 1.22, but for now let\u0026rsquo;s disable it:\nsysctl vm.swappiness=0 swapoff -a It might be worth experimenting with swap enabled in the future to see how that might affect performance.\nEnable IP Forwarding To avoid IP Forwarding issues with Traefik, run the following on the host:\nsudo sysctl net.ipv4.ip_forward=1 sudo sysctl net.ipv6.conf.all.forwarding=1 sudo sed -i \u0026#39;s/#net.ipv4.ip_forward=1/net.ipv4.ip_forward=1/g\u0026#39; /etc/sysctl.conf sudo sed -i \u0026#39;s/#net.ipv6.conf.all.forwarding=1/net.ipv6.conf.all.forwarding=1/g\u0026#39; /etc/sysctl.conf Create LXC container Create an LXC container in the Proxmox interface as you normally would. Remember to:\nUncheck unprivileged container Use a LXC template (I chose a debian 11 template downloaded with pveam) In memory, set swap to 0 Create and start the container Modify container config Now back on the host run pct list to determine what VMID it was given.\nOpen /etc/pve/lxc/$VMID.conf and append:\nlxc.apparmor.profile: unconfined lxc.cap.drop: lxc.mount.auto: \u0026#34;proc:rw sys:rw\u0026#34; lxc.cgroup2.devices.allow: c 10:200 rwm All of the above configurations are described in the manpages. Notice that cgroup2 is used since Proxmox VE 7.0 has switched to a pure cgroupv2 environment.\nThankfully cgroup v2 support has been supported in k3s with these contributions:\nhttps://github.com/k3s-io/k3s/pull/2584 https://github.com/k3s-io/k3s/pull/2844 Enable shared host mounts From within the container, run:\necho \u0026#39;#!/bin/sh -e ln -s /dev/console /dev/kmsg mount --make-rshared /\u0026#39; \u0026gt; /etc/rc.local chmod +x /etc/rc.local reboot Install K3s One of the simplest ways to install K3s on a remote host is to use k3sup. Ensure that you supply a valid CONTAINER_IP and choose the k3s-version you prefer. As of 2021/11, it is still defaulting to the 1.19 channel, so I overrode it to 1.22 for cgroup v2 support. See the published releases here.\nssh-copy-id root@$CONTAINER_IP k3sup install --ip $CONTAINER_IP --user root --k3s-version v1.22.3+k3s1 If all goes well, you should see a path to the kubeconfig generated. I moved this into ~/.kube/config so that kubectl would read this by default.\nWrapping up Installing K3s in LXC on Proxmox works with a few tweaks to the default configuration. I later followed the Tekton\u0026rsquo;s Getting Started guide and was able to deploy it in a few commands.\n$ kubectl get all --namespace tekton-pipelines NAME READY STATUS RESTARTS AGE pod/tekton-pipelines-webhook-8566ff9b6b-6rnh8 1/1 Running 1 (50m ago) 12h pod/tekton-dashboard-6bf858f977-qt4hr 1/1 Running 1 (50m ago) 11h pod/tekton-pipelines-controller-69fd7498d8-f57m4 1/1 Running 1 (50m ago) 12h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/tekton-pipelines-controller ClusterIP 10.43.44.245 \u0026lt;none\u0026gt; 9090/TCP,8080/TCP 12h service/tekton-pipelines-webhook ClusterIP 10.43.183.242 \u0026lt;none\u0026gt; 9090/TCP,8008/TCP,443/TCP,8080/TCP 12h service/tekton-dashboard ClusterIP 10.43.87.97 \u0026lt;none\u0026gt; 9097/TCP 11h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/tekton-pipelines-webhook 1/1 1 1 12h deployment.apps/tekton-dashboard 1/1 1 1 11h deployment.apps/tekton-pipelines-controller 1/1 1 1 12h NAME DESIRED CURRENT READY AGE replicaset.apps/tekton-pipelines-webhook-8566ff9b6b 1 1 1 12h replicaset.apps/tekton-dashboard-6bf858f977 1 1 1 11h replicaset.apps/tekton-pipelines-controller-69fd7498d8 1 1 1 12h NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE horizontalpodautoscaler.autoscaling/tekton-pipelines-webhook Deployment/tekton-pipelines-webhook 9%/100% 1 5 1 12h I made sure to install Tailscale in the container so that I can easily access K3s from anywhere.\nIf I\u0026rsquo;m feeling adventurous, I might experiment with K3s rootless.\n","href":"/blog/2021/11/14/running-k3s-in-lxc-on-proxmox/","title":"Running K3s in LXC on Proxmox"},{"content":"","href":"/tags/containers/","title":"containers"},{"content":"","href":"/tags/docker/","title":"docker"},{"content":"","href":"/tags/podman/","title":"podman"},{"content":"There are a number of reasons why you might want to replace docker, especially on macOS. The following feature bundled in Docker Desktop might have motivated you enough to consider replacing docker:\n...ignoring Docker updates is a paid feature now?? pic.twitter.com/ZxKW3b9LQM\n\u0026mdash; Brendan Dolan-Gavitt (@moyix) May 1, 2021 Docker has been one of the larger influencers in the container world, helping to standardize the OCI Image Format Specification. For many developers, containers have become synonymous with terms like docker and Dockerfile (a file containing build instructions for a container image). Docker has certainly made it very convenient to build and run containers, but it is not the only solution for doing so.\nThis post briefly describes my experience swapping out docker for podman on macOS.\nWhat is a container? A container is a standard unit of software that packages up all application dependencies within it. Multiple containers can be run on a host machine all sharing the same kernel as the host. Linux namespaces help provide an isolated view of the system, including mnt, pid, net, ipc, uid, cgroup, and time. There is an in-depth video that discusses what containers are made from, and near the end there is a demonstration on how to build your own containers from the command line.\nBy easily allowing the necessary dependencies to live alongside the application code, containers make the \u0026ldquo;works on my machine\u0026rdquo; problem less of a problem.\nBenefits of Podman One of the most interesting features of Podman is that it is daemonless. There isn\u0026rsquo;t a process running on your system managing your containers. In contrast, the docker client is reliant upon the docker daemon (often running as root) to be able to build and run containers.\nPodman is rootless by default. It is now possible to run the docker daemon rootless as well, but it\u0026rsquo;s still not the default behaviour.\nI\u0026rsquo;ve also observed that so far my 2019 16\u0026quot; Macbook Pro hasn\u0026rsquo;t sounded like a jet engine, although I haven\u0026rsquo;t performed any disk-intensive operations yet.\nInstalling Podman Running Podman on macOS is more involved than on Linux, because the podman-machine must run Linux inside of a virtual machine. Fortunately, the installation is made simple with brew (read this if you\u0026rsquo;re installing Podman on Linux):\nbrew install podman The podman-machine must be started:\n# This is not necessary on Linux podman machine init podman machine start Running a container Let\u0026rsquo;s try to pull an image:\n$ podman pull alpine Trying to pull docker.io/library/alpine:latest... Getting image source signatures Copying blob sha256:a0d0a0d46f8b52473982a3c466318f479767577551a53ffc9074c9fa7035982e Copying config sha256:14119a10abf4669e8cdbdff324a9f9605d99697215a0d21c360fe8dfa8471bab Writing manifest to image destination Storing signatures 14119a10abf4669e8cdbdff324a9f9605d99697215a0d21c360fe8dfa8471bab If you\u0026rsquo;re having an issue pulling images, you may need to remove ~/.docker/config.json or remove the set of auths in the configuration as mentioned here.\nand then run and exec into the container:\n$ podman run --rm -ti alpine Error: error preparing container 99ace1ef8a78118e178372d91fd182e8166c399fbebe0f676af59fbf32ce205b for attach: error configuring network namespace for container 99ace1ef8a78118e178372d91fd182e8166c399fbebe0f676af59fbf32ce205b: error adding pod unruffled_bohr_unruffled_bohr to CNI network \u0026#34;podman\u0026#34;: unexpected end of JSON input What does this error mean? A bit of searching lead to this github issue.\nUntil the fix is released, a workaround is to just specify a port (even when it\u0026rsquo;s not needed):\npodman run -p 4242 --rm -ti alpine If you\u0026rsquo;re reading this from the future, there is a good chance specifying a port won\u0026rsquo;t be needed.\nAnother example of running a container with Podman can be found in the Jellyfin Documentation.\nAliasing docker with podman Force of habit (or other scripts) may have you calling docker. To work around this:\nalias docker=podman podman-compose You may be wondering: what about docker-compose? Well, there claims to be a drop-in replacement for it: podman-compose.\npip3 install --user podman-compose Now let\u0026rsquo;s create a docker-compose.yml file to test:\ncat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; docker-compose.yml version: \u0026#39;2\u0026#39; services: hello_world: image: ubuntu command: [/bin/echo, \u0026#39;Hello world\u0026#39;] EOF Now run:\n$ podman-compose up podman pod create --name=davegallant.github.io --share net 40d61dc6e95216c07d2b21cea6dcb30205bfcaf1260501fe652f05bddf7e595e 0 podman create --name=davegallant.github.io_hello_world_1 --pod=davegallant.github.io -l io.podman.compose.config-hash=123 -l io.podman.compose.project=davegallant.github.io -l io.podman.compose.version=0.0.1 -l com.docker.compose.container-number=1 -l com.docker.compose.service=hello_world --add-host hello_world:127.0.0.1 --add-host davegallant.github.io_hello_world_1:127.0.0.1 ubuntu /bin/echo Hello world Resolved \u0026#34;ubuntu\u0026#34; as an alias (/etc/containers/registries.conf.d/000-shortnames.conf) Trying to pull docker.io/library/ubuntu:latest... Getting image source signatures Copying blob sha256:f3ef4ff62e0da0ef761ec1c8a578f3035bef51043e53ae1b13a20b3e03726d17 Copying blob sha256:f3ef4ff62e0da0ef761ec1c8a578f3035bef51043e53ae1b13a20b3e03726d17 Copying config sha256:597ce1600cf4ac5f449b66e75e840657bb53864434d6bd82f00b172544c32ee2 Writing manifest to image destination Storing signatures 1a68b2fed3fdf2037b7aef16d770f22929eec1d799219ce30541df7876918576 0 podman start -a davegallant.github.io_hello_world_1 Hello world This should more or less provide the same results you would come to expect with docker. The README does clearly state that podman-compose is under development.\nSummary Installing Podman on macOS was not seamless, but it was manageable well within 30 minutes. I would recommend giving Podman a try to anyone who is unhappy with experiencing forced docker updates, or who is interested in using a more modern technology for running containers.\nOne caveat to mention is that there isn\u0026rsquo;t an official graphical user interface for Podman, but there is an open issue considering one. If you rely heavily on Docker Desktop\u0026rsquo;s UI, you may not be as interested in using podman yet.\nUpdate: After further usage, bind mounts do not seem to work out of the box when the client and host are on different machines. A rather involved solution using sshfs was shared here.\nI had been experimenting with Podman on Linux before writing this, but after listening to this podcast episode, I was inspired to give Podman a try on macOS.\n","href":"/blog/2021/10/11/replacing-docker-with-podman-on-macos-and-linux/","title":"Replacing docker with podman on macOS (and Linux)"},{"content":"Rotating credentials is a security best practice. This morning, I read a question about automatically rotating AWS Access Keys without having to go through the hassle of navigating the AWS console. There are some existing solutions already, but I decided to write a script since it was incredibly simple. The script could be packed up as a systemd/launchd service to continually rotate access keys in the background.\nIn the longer term, migrating my local workflows to aws-vault seems like a more secure solution. This would mean that credentials (even temporary session credentials) never have to be written in plaintext to disk (i.e. where AWS suggests). Any existing applications, such as terraform, could be have their credentials passed to them from aws-vault, which retrieves them from the OS\u0026rsquo;s secure keystore. There is even a rotate command included.\n","href":"/blog/2021/09/17/automatically-rotating-aws-access-keys/","title":"Automatically Rotating AWS Access Keys"},{"content":"","href":"/tags/aws/","title":"aws"},{"content":"","href":"/tags/aws-vault/","title":"aws-vault"},{"content":"","href":"/tags/python/","title":"python"},{"content":"","href":"/tags/security/","title":"security"},{"content":"","href":"/tags/dotfiles/","title":"dotfiles"},{"content":"","href":"/tags/home-manager/","title":"home-manager"},{"content":"","href":"/tags/nix/","title":"nix"},{"content":"Over the years I have collected a number of dotfiles that I have shared across both Linux and macOS machines (~/.zshrc, ~/.config/git/config, ~/.config/tmux/tmux.conf, etc). I have tried several different ways to manage them, including bare git repos and utilities such as GNU Stow. These solutions work well enough, but I have since found what I would consider a much better solution for organizing user configuration: home-manager.\nWhat is home-manager? Before understanding home-manager, it is worth briefly discussing what nix is. nix is a package manager that originally spawned from a PhD thesis. Unlike other package managers, it uses symbolic links to keep track of the currently installed packages, keeping around the old ones in case you may want to rollback.\nFor example, I have used nix to install the package bind which includes dig. You can see that it is available on multiple platforms. The absolute path of dig can be found by running:\n$ ls -lh $(which dig) lrwxr-xr-x 73 root 31 Dec 1969 /run/current-system/sw/bin/dig -\u0026gt; /nix/store/0r4qdyprljd3dki57jn6c6a8dh2rbg9g-bind-9.16.16-dnsutils/bin/dig Notice that there is a hash included in the file path? This is a nix store path and is computed by the nix package manager. This nix pill does a good job explaining how this hash is computed. All of the nix pills are worth a read, if you are interested in learning more about nix itself. However, using home-manager does not require extensive knowledge of nix.\nPart of the nix ecosystem includes nixpkgs. Many popular tools can be found already packaged in this repository. As you can see with these stats, there is a large number of existing packages that are being maintained by the community. Contributing a new package is easy, and anyone can do it!\nhome-manager leverages the nix package manager (and nixpkgs), as well the nix language so that you can declaratively define your system configuration. I store my nix-config in git so that I can keep track of my packages and configurations, and retain a clean and informative git commit history so that I can understand what changed and why.\nSetting up home-manager ⚠️ If you run this on your main machine, make sure you backup your configuration files first. home-manager is pretty good about not overwriting existing configuration, but it is better to have a backup! Alternatively, you could test this out on a VM or cloud instance.\nThe first thing you should do is install nix:\ncurl -L https://nixos.org/nix/install | sh It\u0026rsquo;s generally not a good idea to curl and execute files from the internet (without verifying integrity), so you might want to download the install script first and take a look before executing it!\nOpen up a new shell in your terminal and running nix should work. If not, run . ~/.nix-profile/etc/profile.d/nix.sh\nNow, install home-manager:\nnix-channel --add https://github.com/nix-community/home-manager/archive/master.tar.gz home-manager nix-channel --update nix-shell \u0026#39;\u0026lt;home-manager\u0026gt;\u0026#39; -A install You should see a wave of /nix/store/* paths being displayed on your screen.\nNow, to start off with a basic configuration, open up ~/.config/nixpkgs/home.nix in the editor of your choice and paste this in (you will want to change userName and homeDirectory):\n{ config, pkgs, ... }: { programs.home-manager.enable = true; home = { username = \u0026#34;dave\u0026#34;; homeDirectory = \u0026#34;/home/dave\u0026#34;; stateVersion = \u0026#34;21.11\u0026#34;; packages = with pkgs; [ bind exa fd ripgrep ]; }; programs = { git = { enable = true; aliases = { aa = \u0026#34;add -A .\u0026#34;; br = \u0026#34;branch\u0026#34;; c = \u0026#34;commit -S\u0026#34;; ca = \u0026#34;commit -S --amend\u0026#34;; cb = \u0026#34;checkout -b\u0026#34;; co = \u0026#34;checkout\u0026#34;; d = \u0026#34;diff\u0026#34;; l = \u0026#34;log --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34;; }; delta = { enable = true; options = { features = \u0026#34;line-numbers decorations\u0026#34;; whitespace-error-style = \u0026#34;22 reverse\u0026#34;; plus-style = \u0026#34;green bold ul \u0026#39;#198214\u0026#39;\u0026#34;; decorations = { commit-decoration-style = \u0026#34;bold yellow box ul\u0026#34;; file-style = \u0026#34;bold yellow ul\u0026#34;; file-decoration-style = \u0026#34;none\u0026#34;; }; }; }; extraConfig = { push = { default = \u0026#34;current\u0026#34;; }; pull = { rebase = true; }; }; }; starship = { enable = true; enableZshIntegration = true; settings = { add_newline = false; scan_timeout = 10; }; }; zsh = { enable = true; enableAutosuggestions = true; enableSyntaxHighlighting = true; history.size = 1000000; localVariables = { CASE_SENSITIVE = \u0026#34;true\u0026#34;; DISABLE_UNTRACKED_FILES_DIRTY = \u0026#34;true\u0026#34;; RPROMPT = \u0026#34;\u0026#34;; # override because macOS defaults to filepath ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE = \u0026#34;fg=#838383,underline\u0026#34;; ZSH_DISABLE_COMPFIX = \u0026#34;true\u0026#34;; }; initExtra = \u0026#39;\u0026#39; export PAGER=less \u0026#39;\u0026#39;; shellAliases = { \u0026#34;..\u0026#34; = \u0026#34;cd ..\u0026#34;; grep = \u0026#34;rg --smart-case\u0026#34;; ls = \u0026#34;exa -la --git\u0026#34;; }; \u0026#34;oh-my-zsh\u0026#34; = { enable = true; plugins = [ \u0026#34;gitfast\u0026#34; \u0026#34;last-working-dir\u0026#34; ]; }; }; }; } Save the file and run:\nhome-manager switch You should see another wave of /nix/store/* paths. The new configuration should now be active.\nIf you run zsh, you should see that you have starship and access to several other utils such as rg, fd, and exa.\nThis basic configuration above is also defining your ~/.config/git/config and .zshrc. If you already have either of these files, home-manager will complain about them already existing.\nIf you run cat ~/.zshrc, you will see the way these configuration files are generated.\nYou can extend this configuration for programs such as (neo)vim, emacs, alacritty, ssh, etc. To see other programs, take a look at home-manager/modules/programs.\nGateway To Nix In ways, home-manager can be seen as a gateway to the nix ecosystem. If you have enjoyed the way you can declare user configuration with home-manager, you may be interested in expanding your configuration to include other system dependencies and configuration. For example, in Linux you can define your entire system\u0026rsquo;s configuration (including the kernel, kernel modules, networking, filesystems, etc) in nix. For macOS, there is nix-darwin that includes nix modules for configuring launchd, dock, and other preferences and services. You may also want to check out Nix Flakes: a more recent feature that allows you declare dependencies, and have them automatically pinned and hashed in flake.lock, similar to that of many modern package managers.\nWrapping up The title of this post is slightly misleading, since it\u0026rsquo;s possible to retain some of your dotfiles and have them intermingle with home-manager by including them alongside nix. The idea of defining user configuration using nix can provide a clean way to maintain your configuration, and allow it to be portable across platforms. Is it worth the effort to migrate away from shell scripts and dotfiles? I\u0026rsquo;d say so.\n","href":"/blog/2021/09/08/why-i-threw-out-my-dotfiles/","title":"Why I Threw Out My Dotfiles"},{"content":"","href":"/tags/adguard/","title":"adguard"},{"content":"","href":"/tags/grafana/","title":"grafana"},{"content":"","href":"/tags/homelab/","title":"homelab"},{"content":"","href":"/tags/jellyfin/","title":"jellyfin"},{"content":"","href":"/tags/netdata/","title":"netdata"},{"content":"","href":"/tags/pihole/","title":"pihole"},{"content":"","href":"/tags/plex/","title":"plex"},{"content":"","href":"/tags/tailscale/","title":"tailscale"},{"content":"","href":"/tags/virtualization/","title":"virtualization"},{"content":"A homelab can be an inexpensive way to host a multitude of internal/external services and learn a lot in the process.\nDo you want host your own Media server? Ad blocker? Web server? Are you interested in learning more about Linux? Virtualization? Networking? Security? Building a homelab can be an entertaining playground to enhance your computer skills.\nOne of the best parts about building a homelab is that it doesn\u0026rsquo;t have to be a large investment in terms of hardware. One of the simplest ways to build a homelab is out of a refurbished computer. Having multiple machines/nodes provides the advantage of increased redundancy, but starting out with a single node is enough to reap many of the benefits of having a homelab.\nVirtualization Virtualizing your hardware is an organized way of dividing up your machine\u0026rsquo;s resources. This can be done with something such as a Virtual Machine or something lighter like a container using LXC or runC. Containers have much less overhead in terms of boot time and storage allocation. This Stack Overflow answer sums it up nicely.\nA hypervisor such as Proxmox can be installed in minutes on a new machine. It provides a web interface and a straight-forward way to spin up new VMs and containers. Even if your plan is to run mostly docker containers, Proxmox can be a useful abstraction for managing VMs, disks and running scheduled backups. You can even run docker within an LXC container by enabling nested virtualization. You\u0026rsquo;ll want to ensure that VT-d and VT-x are enabled in the BIOS if you decide to install a hypervisor to manage your virtualization.\nServices So what are some useful services to deploy?\nJellyfin or Plex - basically a self-hosted Netflix that can be used to stream from multiple devices, and the best part is that you manage the content! Unlike Plex, Jellyfin is open source and can be found here. changedetection - is a self-hosted equivalent to something like visualping.io that will notify you when a webpage changes and keep track of the diffs Adguard or Pihole - can block a list of known trackers for all clients on your local network. I\u0026rsquo;ve used pihole for a long time, but have recently switched to Adguard since the UI is more modern and it has the ability to toggle on/off a pre-defined list of services, including Netflix (this is useful if you have stealthy young kids). Either of these will speed up your internet experience, simply because you won\u0026rsquo;t need to download all of the extra tracking bloat. Gitea - A lightweight git server. I use this to mirror git repos from GitHub, GitLab, etc. Homer - A customizable landing page for services you need to access (including the ability to quickly search). Uptime Kuma - A fancy tool for monitoring the uptime of services. There is a large number of services you can self-host, including your own applications that you might be developing. awesome-self-hosted provides a curated list of services that might be of interest to you.\nVPN You could certainly setup and manage your own VPN by using something like OpenVPN, but there is also something else you can try: tailscale. It is a very quick way to create fully-encrypted connections between clients. With its MagicDNS, your can reference the names of machines like homer rather than using an IP address. By using this mesh-like VPN, you can easily create a secure tunnel to your homelab from anywhere.\nMonitoring Monitoring can become an important aspect of your homelab after it starts to become something that is relied upon. One of the simplest ways to setup some monitoring is using netdata. It can be installed on individual containers, VMs, and also a hypervisor (such as Proxmox). All of the monitoring works out of the box by detecting disks, memory, network interfaces, etc.\nAdditionally, agents installed on different machines can all be centrally viewed in netdata, and it can alert you when some of your infrastructure is down or in a degraded state. Adding additional nodes to netdata is as simple as a 1-line shell command.\nAs mentioned above, Uptime Kuma is a convenient way to track uptime and monitor the availability of your services.\nIn Summary Building out a homelab can be a rewarding experience and it doesn\u0026rsquo;t require buying a rack full of expensive servers to get a significant amount of utility. There are many services that you can run that require very minimal setup, making it possible to get a server up and running in a short period of time, with monitoring, and that can be securely connected to remotely.\n","href":"/blog/2021/09/06/what-to-do-with-a-homelab/","title":"What To Do With A Homelab"},{"content":"AppGate SDP provides a Zero Trust network. This post describes how to get AppGate SDP 4.3.2 working on Arch Linux.\nDepending on the AppGate SDP Server that is running, you may require a client that is more recent than the latest package on AUR. As of right now, the latest AUR is 4.2.2-1.\nThese steps highlight how to get it working with Python3.8 by making a 1 line modification to AppGate source code.\nPackaging We already know the community package is currently out of date, so let\u0026rsquo;s clone it:\ngit clone https://aur.archlinux.org/appgate-sdp.git cd appgate-sdp You\u0026rsquo;ll likely notice that the version is not what we want, so let\u0026rsquo;s modify the PKGBUILD to the following:\n# Maintainer: Pawel Mosakowski \u0026lt;pawel at mosakowski dot net\u0026gt; pkgname=appgate-sdp conflicts=(\u0026#39;appgate-sdp-headless\u0026#39;) pkgver=4.3.2 _download_pkgver=4.3 pkgrel=1 epoch= pkgdesc=\u0026#34;Software Defined Perimeter - GUI client\u0026#34; arch=(\u0026#39;x86_64\u0026#39;) url=\u0026#34;https://www.cyxtera.com/essential-defense/appgate-sdp/support\u0026#34; license=(\u0026#39;custom\u0026#39;) # dependecies calculated by namcap depends=(\u0026#39;gconf\u0026#39; \u0026#39;libsecret\u0026#39; \u0026#39;gtk3\u0026#39; \u0026#39;python\u0026#39; \u0026#39;nss\u0026#39; \u0026#39;libxss\u0026#39; \u0026#39;nodejs\u0026#39; \u0026#39;dnsmasq\u0026#39;) source=(\u0026#34;https://sdpdownloads.cyxtera.com/AppGate-SDP-${_download_pkgver}/clients/${pkgname}_${pkgver}_amd64.deb\u0026#34; \u0026#34;appgatedriver.service\u0026#34;) options=(staticlibs) prepare() { tar -xf data.tar.xz } package() { cp -dpr \u0026#34;${srcdir}\u0026#34;/{etc,lib,opt,usr} \u0026#34;${pkgdir}\u0026#34; mv -v \u0026#34;$pkgdir/lib/systemd/system\u0026#34; \u0026#34;$pkgdir/usr/lib/systemd/\u0026#34; rm -vrf \u0026#34;$pkgdir/lib\u0026#34; cp -v \u0026#34;$srcdir/appgatedriver.service\u0026#34; \u0026#34;$pkgdir/usr/lib/systemd/system/appgatedriver.service\u0026#34; mkdir -vp \u0026#34;$pkgdir/usr/share/licenses/appgate-sdp\u0026#34; cp -v \u0026#34;$pkgdir/usr/share/doc/appgate/copyright\u0026#34; \u0026#34;$pkgdir/usr/share/licenses/appgate-sdp\u0026#34; cp -v \u0026#34;$pkgdir/usr/share/doc/appgate/LICENSE.github\u0026#34; \u0026#34;$pkgdir/usr/share/licenses/appgate-sdp\u0026#34; cp -v \u0026#34;$pkgdir/usr/share/doc/appgate/LICENSES.chromium.html.bz2\u0026#34; \u0026#34;$pkgdir/usr/share/licenses/appgate-sdp\u0026#34; } md5sums=(\u0026#39;17101aac7623c06d5fbb95f50cf3dbdc\u0026#39; \u0026#39;002644116e20b2d79fdb36b7677ab4cf\u0026#39;) Let\u0026rsquo;s first make sure we have some dependencies. If you do not have yay, check it out.\nyay -S dnsmasq gconf Now, let\u0026rsquo;s install it:\nmakepkg -si Running the client Ok, let\u0026rsquo;s run the client by executing appgate.\nIt complains about not being able to connect.\nEasy fix:\nsudo systemctl start appgatedriver.service Now we should be connected\u0026hellip; but DNS is not working?\nFixing the DNS Running resolvectl should display that something is not right.\nWhy is the DNS not being set by appgate?\n$ head -3 /opt/appgate/linux/set_dns #!/usr/bin/env python3 \u0026#39;\u0026#39;\u0026#39; This is used to set and unset the DNS. It seems like python3 is required for the DNS setting to happen. Let\u0026rsquo;s try to run it.\n$ sudo /opt/appgate/linux/set_dns /opt/appgate/linux/set_dns:88: SyntaxWarning: \u0026#34;is\u0026#34; with a literal. Did you mean \u0026#34;==\u0026#34;? servers = [( socket.AF_INET if x.version is 4 else socket.AF_INET6, map(int, x.packed)) for x in servers] Traceback (most recent call last): File \u0026#34;/opt/appgate/linux/set_dns\u0026#34;, line 30, in \u0026lt;module\u0026gt; import dbus ModuleNotFoundError: No module named \u0026#39;dbus\u0026#39; Ok, let\u0026rsquo;s install it:\n$ sudo python3.8 -m pip install dbus-python Will it work now? Not yet. There\u0026rsquo;s another issue:\n$ sudo /opt/appgate/linux/set_dns /opt/appgate/linux/set_dns:88: SyntaxWarning: \u0026#34;is\u0026#34; with a literal. Did you mean \u0026#34;==\u0026#34;? servers = [( socket.AF_INET if x.version is 4 else socket.AF_INET6, map(int, x.packed)) for x in servers] module \u0026#39;platform\u0026#39; has no attribute \u0026#39;linux_distribution\u0026#39; This is a breaking change in Python3.8.\nSo what is calling platform.linux_distribution?\nLet\u0026rsquo;s search for it:\n$ sudo grep -r \u0026#39;linux_distribution\u0026#39; /opt/appgate/linux/ /opt/appgate/linux/nm.py: if platform.linux_distribution()[0] != \u0026#39;Fedora\u0026#39;: Aha! So this is in the local AppGate source code. This should be an easy fix. Let\u0026rsquo;s just replace this line with:\nif True: # Since we are not using Fedora :) Wrapping up It turns out there are breaking changes in Python3.8.\nThe docs say Deprecated since version 3.5, will be removed in version 3.8: See alternative like the distro package.\nI suppose this highlights one of the caveats of relying upon the system\u0026rsquo;s python, rather than having an isolated, dedicated environment for all dependencies.\n","href":"/blog/2020/03/16/appgate-sdp-on-arch-linux/","title":"AppGate SDP on Arch Linux"},{"content":"","href":"/tags/linux/","title":"linux"},{"content":"","href":"/tags/vpn/","title":"vpn"},{"content":"test\n","href":"/page/search/","title":""},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/page/","title":"Pages"}]
